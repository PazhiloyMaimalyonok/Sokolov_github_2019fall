{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение, ФКН ВШЭ\n",
    "\n",
    "## Практическое задание 3. Градиентный спуск своими руками\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: 05.10.2019\n",
    "\n",
    "Мягкий дедлайн: 07:59MSK 15.10.2019 (за каждый день просрочки снимается 1 балл)\n",
    "\n",
    "Жесткий дедлайн: 23:59MSK 17.10.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска.\n",
    "\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "\n",
    "### Формат сдачи\n",
    "Задания сдаются через систему Anytask. Инвайт можно найти на странице курса. Присылать необходимо ноутбук с выполненным заданием. \n",
    "\n",
    "Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.\n",
    "\n",
    "**Оценка**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация градиентного спуска\n",
    "\n",
    "Реализуйте линейную регрессию с функцией потерь MSE, обучаемую с помощью:\n",
    "\n",
    "** Задание 1 (1 балл)** Градиентного спуска;\n",
    "\n",
    "** Задание 2 (1.5 балла)** Стохастического градиентного спуска;\n",
    "\n",
    "** Задание 3 (2.5 балла)** Метода Momentum.\n",
    "\n",
    "\n",
    "Во всех пунктах необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы;\n",
    "* Циклы средствами python допускается использовать только для итераций градиентного спуска;\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "\n",
    "    * проверку на евклидовую норму разности весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$, задаваемого параметром `tolerance`);\n",
    "    * достижение максимального числа итераций (например, 10000, задаваемого параметром `max_iter`).\n",
    "* Чтобы проследить, что оптимизационный процесс действительно сходится, будем использовать атрибут класса `loss_history` — в нём после вызова метода `fit` должны содержаться значения функции потерь для всех итераций, начиная с первой (до совершения первого шага по антиградиенту);\n",
    "* Инициализировать веса можно случайным образом или нулевым вектором. \n",
    "\n",
    "\n",
    "Ниже приведён шаблон класса, который должен содержать код реализации каждого из методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearReg(BaseEstimator):\n",
    "    def __init__(self, gd_type='stochastic', \n",
    "                 tolerance=1e-7, max_iter=5000, w0=None, alpha=1e-3, eta=1e-3):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic' or 'momentum'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        if self.w0 == None:\n",
    "            self.w0 = np.zeros(X.shape[1])\n",
    "        self.w = self.w0\n",
    "        self.loss_history.append(self.calc_loss(X, y))\n",
    "        if self.gd_type == 'full':\n",
    "            for i in range(self.max_iter):\n",
    "                #gradient step and new weights\n",
    "                grad_step = self.eta*self.calc_gradient(X, y)\n",
    "                self.w = self.w - grad_step\n",
    "                #append loss history\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                #checking tolerance for weights\n",
    "                if (grad_step**2).sum() < self.tolerance:\n",
    "                    break\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return X.dot(self.w)\n",
    "        pass\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d) (ell can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        A = X.dot(self.w) - y\n",
    "        grad = X.T.dot(A)*2/X.shape[0]\n",
    "        return grad\n",
    "        pass\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\" \n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return ((X.dot(self.w) - y)**2).sum()/X.shape[0]\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 4 (0 баллов)**. \n",
    "* Загрузите данные из домашнего задания 2 ([train.csv](https://www.kaggle.com/c/nyc-taxi-trip-duration/data));\n",
    "* Разбейте выборку на обучающую и тестовую в отношении 7:3 с random_seed=0;\n",
    "* Преобразуйте целевую переменную `trip_duration` как $\\hat{y} = \\log{(y + 1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nscaler.fit(X_train)\\nX_train = scaler.transform(X_train)\\nX_test = scaler.transform(X_test)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "import pandas as pd\n",
    "import math\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train['log_trip_duration'] = np.log(df_train['trip_duration'] + 1)\n",
    "\n",
    "#Data cleaning\n",
    "df_train.drop('id',inplace=True,axis=1)\n",
    "df_train.drop('dropoff_datetime',inplace=True,axis=1)\n",
    "df_train[\"pickup_datetime\"]=pd.to_datetime(df_train[\"pickup_datetime\"])\n",
    "df_train['day_of_week'] = df_train['pickup_datetime'].dt.day_name()\n",
    "df_train['hour_of_the_day']=df_train['pickup_datetime'].dt.hour\n",
    "df_train['month']=df_train['pickup_datetime'].dt.month\n",
    "df_train['day_of_week']=df_train['day_of_week'].map({'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6,'Sunday':7})\n",
    "df_train['store_and_fwd_flag']=df_train['store_and_fwd_flag'].map({'N':0,'Y':1})\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # distance between latitudes and longitudes\n",
    "    dLat = (lat2 - lat1) * math.pi / 180.0\n",
    "    dLon = (lon2 - lon1) * math.pi / 180.0\n",
    " \n",
    "    # convert to radians\n",
    "    lat1 = (lat1) * math.pi / 180.0\n",
    "    lat2 = (lat2) * math.pi / 180.0\n",
    " \n",
    "    # apply formulae\n",
    "    a = (pow(math.sin(dLat / 2), 2) +\n",
    "         pow(math.sin(dLon / 2), 2) *\n",
    "             math.cos(lat1) * math.cos(lat2));\n",
    "    rad = 6371\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return rad * c\n",
    "df_train['distance']=df_train.apply(lambda row:haversine(row['pickup_latitude'],row['pickup_longitude'],row['dropoff_latitude'],row['dropoff_longitude']),axis=1)\n",
    "df_train['distance']=df_train['distance'].astype(float)\n",
    "df_train.drop('pickup_longitude',inplace=True,axis=1)\n",
    "df_train.drop('pickup_latitude',inplace=True,axis=1)\n",
    "df_train.drop('dropoff_longitude',inplace=True,axis=1)\n",
    "df_train.drop('dropoff_latitude',inplace=True,axis=1)\n",
    "df_train.drop('pickup_datetime',inplace=True,axis=1)\n",
    "\n",
    "#train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train.drop(columns = ['log_trip_duration', 'trip_duration']),\n",
    "                                                            df_train['log_trip_duration'], test_size=0.3, random_state=0)\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571578     6.079971\n",
       "1280332    9.631096\n",
       "177838     5.973925\n",
       "1433776    6.783219\n",
       "757662     4.950731\n",
       "             ...   \n",
       "1042944    9.106721\n",
       "85930      3.485629\n",
       "61268      9.060404\n",
       "251164     7.441664\n",
       "1348190    5.062943\n",
       "Length: 437594, dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearReg(gd_type = 'full')\n",
    "reg.fit(X_train, y_train)\n",
    "reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.8948393371265615,\n",
       " 1.8947384327650874,\n",
       " 1.8946376561764315,\n",
       " 1.8945370071983676,\n",
       " 1.8944364856688756,\n",
       " 1.8943360914261427,\n",
       " 1.8942358243085646,\n",
       " 1.8941356841547443,\n",
       " 1.8940356708034902,\n",
       " 1.8939357840938191]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.loss_history[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41452899996385256"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "((reg.predict(X_train) - y_train)**2).sum()/X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42.45540471238833,\n",
       " 38.64401536143907,\n",
       " 35.224830961978626,\n",
       " 32.15733964317357,\n",
       " 29.40521500209197,\n",
       " 26.935883677692118,\n",
       " 24.72013760140298,\n",
       " 22.731786308483194,\n",
       " 20.9473451712318,\n",
       " 19.345755842743614,\n",
       " 17.90813558333912,\n",
       " 16.617552485622053,\n",
       " 15.45882392241763,\n",
       " 14.418335818292851,\n",
       " 13.483880593246166,\n",
       " 12.64451184942976,\n",
       " 11.890414071078482,\n",
       " 11.212785786538827,\n",
       " 10.603734801545162,\n",
       " 10.05618425658858,\n",
       " 9.563788390074578,\n",
       " 9.120857004504693,\n",
       " 8.72228773651926,\n",
       " 8.363505324536117,\n",
       " 8.040407151020768,\n",
       " 7.749314411116982,\n",
       " 7.4869283263440485,\n",
       " 7.2502908821236876,\n",
       " 7.036749621751972,\n",
       " 6.843926077719976,\n",
       " 6.669687464586187,\n",
       " 6.512121296429796,\n",
       " 6.369512626728346,\n",
       " 6.2403236397209625,\n",
       " 6.123175350310776,\n",
       " 6.016831194660236,\n",
       " 5.920182316140448,\n",
       " 5.832234371476822,\n",
       " 5.752095700030398,\n",
       " 5.678966715380832,\n",
       " 5.6121303929276065,\n",
       " 5.550943740272994,\n",
       " 5.494830148849694,\n",
       " 5.4432725357462255,\n",
       " 5.395807194089961,\n",
       " 5.352018278782351,\n",
       " 5.311532861944209,\n",
       " 5.27401649921083,\n",
       " 5.239169254097896,\n",
       " 5.206722133112097,\n",
       " 5.176433889169922,\n",
       " 5.148088155272421,\n",
       " 5.121490874315197,\n",
       " 5.096467994438129,\n",
       " 5.072863402480226,\n",
       " 5.050537070939596,\n",
       " 5.029363396379944,\n",
       " 5.009229709504182,\n",
       " 4.990034939159108,\n",
       " 4.971688414367665,\n",
       " 4.954108790128254,\n",
       " 4.937223084194061,\n",
       " 4.920965813366302,\n",
       " 4.905278219020079,\n",
       " 4.890107572643673,\n",
       " 4.875406553124551,\n",
       " 4.8611326883696195,\n",
       " 4.847247854612901,\n",
       " 4.833717827450583,\n",
       " 4.820511879259277,\n",
       " 4.807602418205274,\n",
       " 4.7949646645478605,\n",
       " 4.78257636038358,\n",
       " 4.770417509376503,\n",
       " 4.758470143376456,\n",
       " 4.746718113147318,\n",
       " 4.735146900714318,\n",
       " 4.723743451096952,\n",
       " 4.712496021424476,\n",
       " 4.701394045638253,\n",
       " 4.690428013170492,\n",
       " 4.679589360155494,\n",
       " 4.668870371878508,\n",
       " 4.658264095301281,\n",
       " 4.647764260623194,\n",
       " 4.637365210944425,\n",
       " 4.627061839194202,\n",
       " 4.616849531573457,\n",
       " 4.606724116838859,\n",
       " 4.5966818208248235,\n",
       " 4.586719225662212,\n",
       " 4.5768332332086175,\n",
       " 4.567021032255051,\n",
       " 4.557280069118865,\n",
       " 4.547608021273155,\n",
       " 4.538002773698871,\n",
       " 4.528462397678353,\n",
       " 4.518985131778104,\n",
       " 4.509569364794605,\n",
       " 4.500213620460359,\n",
       " 4.490916543728378,\n",
       " 4.481676888471926,\n",
       " 4.472493506453473,\n",
       " 4.463365337431608,\n",
       " 4.454291400288423,\n",
       " 4.44527078507193,\n",
       " 4.43630264585899,\n",
       " 4.427386194353982,\n",
       " 4.4185206941472455,\n",
       " 4.409705455565109,\n",
       " 4.40093983105038,\n",
       " 4.392223211018588,\n",
       " 4.383555020140727,\n",
       " 4.374934714008533,\n",
       " 4.3663617761427505,\n",
       " 4.357835715308945,\n",
       " 4.349356063109167,\n",
       " 4.3409223718208745,\n",
       " 4.332534212457691,\n",
       " 4.324191173029009,\n",
       " 4.315892856977941,\n",
       " 4.307638881779207,\n",
       " 4.299428877680438,\n",
       " 4.291262486572068,\n",
       " 4.283139360972591,\n",
       " 4.275059163117228,\n",
       " 4.267021564139328,\n",
       " 4.259026243335017,\n",
       " 4.251072887502358,\n",
       " 4.2431611903474735,\n",
       " 4.235290851950663,\n",
       " 4.227461578286293,\n",
       " 4.219673080790988,\n",
       " 4.211925075975108,\n",
       " 4.204217285073013,\n",
       " 4.196549433728197,\n",
       " 4.188921251709623,\n",
       " 4.181332472656104,\n",
       " 4.1737828338457845,\n",
       " 4.166272075988203,\n",
       " 4.158799943036562,\n",
       " 4.15136618201815,\n",
       " 4.143970542881049,\n",
       " 4.136612778355427,\n",
       " 4.129292643827983,\n",
       " 4.122009897228126,\n",
       " 4.114764298924706,\n",
       " 4.107555611632252,\n",
       " 4.100383600325682,\n",
       " 4.093248032162682,\n",
       " 4.0861486764129085,\n",
       " 4.079085304393381,\n",
       " 4.072057689409385,\n",
       " 4.065065606700337,\n",
       " 4.058108833390139,\n",
       " 4.051187148441508,\n",
       " 4.044300332613953,\n",
       " 4.037448168424949,\n",
       " 4.0306304401140505,\n",
       " 4.023846933609628,\n",
       " 4.017097436497954,\n",
       " 4.0103817379944156,\n",
       " 4.003699628916646,\n",
       " 3.9970509016593745,\n",
       " 3.9904353501708187,\n",
       " 3.983852769930501,\n",
       " 3.977302957928308,\n",
       " 3.970785712644705,\n",
       " 3.9643008340319787,\n",
       " 3.9578481234963983,\n",
       " 3.951427383881242,\n",
       " 3.9450384194505785,\n",
       " 3.9386810358737354,\n",
       " 3.9323550402104055,\n",
       " 3.926060240896326,\n",
       " 3.9197964477294542,\n",
       " 3.9135634718566603,\n",
       " 3.9073611257608127,\n",
       " 3.9011892232482652,\n",
       " 3.8950475794367305,\n",
       " 3.888936010743453,\n",
       " 3.8828543348736964,\n",
       " 3.8768023708095214,\n",
       " 3.8707799387988073,\n",
       " 3.864786860344508,\n",
       " 3.8588229581941524,\n",
       " 3.8528880563295154,\n",
       " 3.8469819799565057,\n",
       " 3.841104555495224,\n",
       " 3.835255610570169,\n",
       " 3.8294349740006193,\n",
       " 3.8236424757911642,\n",
       " 3.8178779471223403,\n",
       " 3.8121412203414327,\n",
       " 3.8064321289533765,\n",
       " 3.8007505076117867,\n",
       " 3.7950961921100843,\n",
       " 3.789469019372741,\n",
       " 3.783868827446622,\n",
       " 3.7782954554924166,\n",
       " 3.77274874377617,\n",
       " 3.767228533660895,\n",
       " 3.7617346675982883,\n",
       " 3.756266989120506,\n",
       " 3.7508253428320275,\n",
       " 3.74540957440161,\n",
       " 3.7400195305542976,\n",
       " 3.7346550590635292,\n",
       " 3.729316008743282,\n",
       " 3.7240022294403246,\n",
       " 3.718713572026515,\n",
       " 3.713449888391173,\n",
       " 3.7082110314334975,\n",
       " 3.7029968550550976,\n",
       " 3.697807214152531,\n",
       " 3.69264196460995,\n",
       " 3.687500963291765,\n",
       " 3.682384068035402,\n",
       " 3.677291137644117,\n",
       " 3.6722220318798464,\n",
       " 3.6671766114561377,\n",
       " 3.6621547380311146,\n",
       " 3.657156274200532,\n",
       " 3.652181083490848,\n",
       " 3.647229030352372,\n",
       " 3.6422999801524747,\n",
       " 3.6373937991688208,\n",
       " 3.6325103545826947,\n",
       " 3.627649514472334,\n",
       " 3.6228111478063645,\n",
       " 3.617995124437237,\n",
       " 3.6132013150947677,\n",
       " 3.6084295913796516,\n",
       " 3.6036798257571294,\n",
       " 3.5989518915506067,\n",
       " 3.5942456629353923,\n",
       " 3.58956101493243,\n",
       " 3.5848978234021214,\n",
       " 3.580255965038178,\n",
       " 3.575635317361517,\n",
       " 3.571035758714212,\n",
       " 3.5664571682534905,\n",
       " 3.5618994259457604,\n",
       " 3.557362412560716,\n",
       " 3.5528460096654544,\n",
       " 3.548350099618657,\n",
       " 3.543874565564804,\n",
       " 3.5394192914284464,\n",
       " 3.5349841619085107,\n",
       " 3.5305690624726527,\n",
       " 3.526173879351648,\n",
       " 3.52179849953383,\n",
       " 3.5174428107595834,\n",
       " 3.5131067015158375,\n",
       " 3.5087900610306604,\n",
       " 3.504492779267844,\n",
       " 3.5002147469215665,\n",
       " 3.4959558554110566,\n",
       " 3.491715996875346,\n",
       " 3.4874950641680162,\n",
       " 3.483292950852021,\n",
       " 3.4791095511945254,\n",
       " 3.4749447601617884,\n",
       " 3.470798473414096,\n",
       " 3.466670587300723,\n",
       " 3.4625609988549266,\n",
       " 3.458469605788994,\n",
       " 3.4543963064893206,\n",
       " 3.4503410000115116,\n",
       " 3.446303586075554,\n",
       " 3.4422839650609873,\n",
       " 3.438282038002132,\n",
       " 3.434297706583362,\n",
       " 3.4303308731343884,\n",
       " 3.426381440625597,\n",
       " 3.4224493126634212,\n",
       " 3.4185343934857304,\n",
       " 3.4146365879572844,\n",
       " 3.4107558015652035,\n",
       " 3.4068919404144618,\n",
       " 3.40304491122345,\n",
       " 3.3992146213195262,\n",
       " 3.3954009786346417,\n",
       " 3.3916038917009765,\n",
       " 3.387823269646605,\n",
       " 3.384059022191227,\n",
       " 3.3803110596418824,\n",
       " 3.3765792928887293,\n",
       " 3.3728636334008595,\n",
       " 3.369163993222125,\n",
       " 3.3654802849669996,\n",
       " 3.361812421816487,\n",
       " 3.3581603175140504,\n",
       " 3.35452388636156,\n",
       " 3.3509030432152986,\n",
       " 3.3472977034819746,\n",
       " 3.343707783114775,\n",
       " 3.3401331986094416,\n",
       " 3.3365738670003884,\n",
       " 3.333029705856844,\n",
       " 3.3295006332790003,\n",
       " 3.3259865678942386,\n",
       " 3.3224874288533313,\n",
       " 3.3190031358267142,\n",
       " 3.3155336090007617,\n",
       " 3.3120787690740983,\n",
       " 3.3086385372539313,\n",
       " 3.3052128352524397,\n",
       " 3.3018015852831386,\n",
       " 3.2984047100573286,\n",
       " 3.2950221327805105,\n",
       " 3.291653777148895,\n",
       " 3.2882995673458884,\n",
       " 3.28495942803861,\n",
       " 3.281633284374469,\n",
       " 3.2783210619777203,\n",
       " 3.2750226869460928,\n",
       " 3.271738085847403,\n",
       " 3.2684671857162124,\n",
       " 3.265209914050528,\n",
       " 3.2619661988084845,\n",
       " 3.2587359684050923,\n",
       " 3.255519151708979,\n",
       " 3.252315678039191,\n",
       " 3.2491254771619764,\n",
       " 3.245948479287624,\n",
       " 3.2427846150673143,\n",
       " 3.239633815589992,\n",
       " 3.2364960123792734,\n",
       " 3.23337113739036,\n",
       " 3.230259123006988,\n",
       " 3.227159902038403,\n",
       " 3.2240734077163355,\n",
       " 3.220999573692032,\n",
       " 3.217938334033286,\n",
       " 3.214889623221485,\n",
       " 3.2118533761487096,\n",
       " 3.2088295281148276,\n",
       " 3.205818014824615,\n",
       " 3.2028187723849033,\n",
       " 3.199831737301746,\n",
       " 3.1968568464776213,\n",
       " 3.193894037208616,\n",
       " 3.1909432471816737,\n",
       " 3.1880044144718465,\n",
       " 3.185077477539549,\n",
       " 3.182162375227875,\n",
       " 3.1792590467598925,\n",
       " 3.176367431735978,\n",
       " 3.1734874701311755,\n",
       " 3.170619102292563,\n",
       " 3.16776226893665,\n",
       " 3.164916911146776,\n",
       " 3.1620829703705615,\n",
       " 3.159260388417333,\n",
       " 3.1564491074556176,\n",
       " 3.15364907001061,\n",
       " 3.1508602189616837,\n",
       " 3.1480824975399297,\n",
       " 3.1453158493256814,\n",
       " 3.1425602182460852,\n",
       " 3.1398155485726846,\n",
       " 3.1370817849190042,\n",
       " 3.1343588722381837,\n",
       " 3.131646755820599,\n",
       " 3.128945381291516,\n",
       " 3.1262546946087584,\n",
       " 3.1235746420604023,\n",
       " 3.1209051702624686,\n",
       " 3.1182462261566504,\n",
       " 3.1155977570080458,\n",
       " 3.112959710402914,\n",
       " 3.1103320342464515,\n",
       " 3.107714676760571,\n",
       " 3.1051075864817075,\n",
       " 3.102510712258643,\n",
       " 3.0999240032503432,\n",
       " 3.0973474089238002,\n",
       " 3.0947808790519136,\n",
       " 3.092224363711368,\n",
       " 3.089677813280536,\n",
       " 3.0871411784373866,\n",
       " 3.084614410157424,\n",
       " 3.082097459711638,\n",
       " 3.0795902786644507,\n",
       " 3.0770928188717064,\n",
       " 3.0746050324786656,\n",
       " 3.0721268719179893,\n",
       " 3.0696582899077876,\n",
       " 3.0671992394496455,\n",
       " 3.064749673826665,\n",
       " 3.0623095466015466,\n",
       " 3.059878811614645,\n",
       " 3.057457422982099,\n",
       " 3.055045335093896,\n",
       " 3.0526425026120263,\n",
       " 3.050248880468611,\n",
       " 3.047864423864038,\n",
       " 3.045489088265136,\n",
       " 3.0431228294033605,\n",
       " 3.040765603272959,\n",
       " 3.0384173661292007,\n",
       " 3.0360780744865705,\n",
       " 3.0337476851170178,\n",
       " 3.031426155048187,\n",
       " 3.029113441561679,\n",
       " 3.0268095021913117,\n",
       " 3.024514294721417,\n",
       " 3.0222277771851256,\n",
       " 3.019949907862664,\n",
       " 3.017680645279692,\n",
       " 3.0154199482056256,\n",
       " 3.013167775651978,\n",
       " 3.0109240868707197,\n",
       " 3.008688841352642,\n",
       " 3.0064619988257464,\n",
       " 3.004243519253632,\n",
       " 3.0020333628338887,\n",
       " 2.999831489996532,\n",
       " 2.997637861402411,\n",
       " 2.995452437941668,\n",
       " 2.9932751807321645,\n",
       " 2.9911060511179666,\n",
       " 2.9889450106677975,\n",
       " 2.98679202117354,\n",
       " 2.984647044648716,\n",
       " 2.982510043326997,\n",
       " 2.9803809796607315,\n",
       " 2.9782598163194587,\n",
       " 2.9761465161884546,\n",
       " 2.974041042367283,\n",
       " 2.971943358168351,\n",
       " 2.96985342711548,\n",
       " 2.9677712129424947,\n",
       " 2.965696679591801,\n",
       " 2.963629791213001,\n",
       " 2.9615705121614933,\n",
       " 2.9595188069971083,\n",
       " 2.957474640482731,\n",
       " 2.95543797758294,\n",
       " 2.953408783462672,\n",
       " 2.9513870234858723,\n",
       " 2.9493726632141763,\n",
       " 2.947365668405577,\n",
       " 2.9453660050131334,\n",
       " 2.943373639183659,\n",
       " 2.9413885372564317,\n",
       " 2.939410665761923,\n",
       " 2.937439991420514,\n",
       " 2.935476481141247,\n",
       " 2.9335201020205575,\n",
       " 2.931570821341045,\n",
       " 2.9296286065702266,\n",
       " 2.92769342535932,\n",
       " 2.925765245542022,\n",
       " 2.9238440351333037,\n",
       " 2.9219297623282072,\n",
       " 2.920022395500656,\n",
       " 2.9181219032022816,\n",
       " 2.9162282541612297,\n",
       " 2.9143414172810247,\n",
       " 2.912461361639389,\n",
       " 2.9105880564871005,\n",
       " 2.9087214712468645,\n",
       " 2.90686157551217,\n",
       " 2.9050083390461703,\n",
       " 2.9031617317805707,\n",
       " 2.9013217238145157,\n",
       " 2.8994882854134936,\n",
       " 2.897661387008249,\n",
       " 2.8958409991936986,\n",
       " 2.8940270927278418,\n",
       " 2.8922196385307153,\n",
       " 2.89041860768332,\n",
       " 2.888623971426571,\n",
       " 2.886835701160252,\n",
       " 2.885053768441984,\n",
       " 2.883278144986189,\n",
       " 2.8815088026630646,\n",
       " 2.879745713497589,\n",
       " 2.8779888496684882,\n",
       " 2.876238183507249,\n",
       " 2.8744936874971305,\n",
       " 2.8727553342721652,\n",
       " 2.871023096616188,\n",
       " 2.869296947461861,\n",
       " 2.8675768598897196,\n",
       " 2.8658628071271997,\n",
       " 2.8641547625476913,\n",
       " 2.8624526996696043,\n",
       " 2.860756592155411,\n",
       " 2.859066413810738,\n",
       " 2.8573821385834304,\n",
       " 2.855703740562634,\n",
       " 2.8540311939778777,\n",
       " 2.8523644731981963,\n",
       " 2.850703552731198,\n",
       " 2.849048407222198,\n",
       " 2.847399011453321,\n",
       " 2.8457553403426257,\n",
       " 2.84411736894324,\n",
       " 2.842485072442478,\n",
       " 2.840858426161004,\n",
       " 2.8392374055519474,\n",
       " 2.8376219862000815,\n",
       " 2.836012143820978,\n",
       " 2.834407854260154,\n",
       " 2.832809093492264,\n",
       " 2.831215837620265,\n",
       " 2.82962806287459,\n",
       " 2.8280457456123638,\n",
       " 2.8264688623165632,\n",
       " 2.8248973895952374,\n",
       " 2.823331304180706,\n",
       " 2.821770582928778,\n",
       " 2.820215202817952,\n",
       " 2.8186651409486565,\n",
       " 2.817120374542459,\n",
       " 2.8155808809413174,\n",
       " 2.814046637606806,\n",
       " 2.81251762211936,\n",
       " 2.810993812177527,\n",
       " 2.8094751855972304,\n",
       " 2.807961720311009,\n",
       " 2.806453394367302,\n",
       " 2.804950185929706,\n",
       " 2.803452073276256,\n",
       " 2.801959034798704,\n",
       " 2.800471049001805,\n",
       " 2.798988094502613,\n",
       " 2.797510150029761,\n",
       " 2.796037194422785,\n",
       " 2.7945692066314027,\n",
       " 2.7931061657148497,\n",
       " 2.7916480508411743,\n",
       " 2.790194841286572,\n",
       " 2.7887465164346987,\n",
       " 2.7873030557760083,\n",
       " 2.7858644389070872,\n",
       " 2.7844306455299863,\n",
       " 2.7830016554515717,\n",
       " 2.78157744858287,\n",
       " 2.7801580049384214,\n",
       " 2.778743304635632,\n",
       " 2.7773333278941528,\n",
       " 2.7759280550352234,\n",
       " 2.7745274664810546,\n",
       " 2.7731315427542063,\n",
       " 2.771740264476963,\n",
       " 2.7703536123707106,\n",
       " 2.7689715672553463,\n",
       " 2.767594110048645,\n",
       " 2.7662212217656665,\n",
       " 2.764852883518167,\n",
       " 2.763489076513989,\n",
       " 2.7621297820564785,\n",
       " 2.760774981543891,\n",
       " 2.7594246564688283,\n",
       " 2.758078788417631,\n",
       " 2.756737359069828,\n",
       " 2.7554003501975575,\n",
       " 2.7540677436649954,\n",
       " 2.7527395214278028,\n",
       " 2.751415665532561,\n",
       " 2.750096158116217,\n",
       " 2.748780981405539,\n",
       " 2.747470117716557,\n",
       " 2.7461635494540326,\n",
       " 2.744861259110914,\n",
       " 2.7435632292677967,\n",
       " 2.7422694425923977,\n",
       " 2.740979881839025,\n",
       " 2.7396945298480464,\n",
       " 2.738413369545385,\n",
       " 2.7371363839419818,\n",
       " 2.735863556133296,\n",
       " 2.734594869298789,\n",
       " 2.7333303067014185,\n",
       " 2.732069851687138,\n",
       " 2.7308134876843835,\n",
       " 2.729561198203603,\n",
       " 2.7283129668367403,\n",
       " 2.727068777256753,\n",
       " 2.725828613217128,\n",
       " 2.7245924585513968,\n",
       " 2.7233602971726607,\n",
       " 2.7221321130731044,\n",
       " 2.720907890323536,\n",
       " 2.719687613072902,\n",
       " 2.718471265547835,\n",
       " 2.717258832052187,\n",
       " 2.716050296966558,\n",
       " 2.7148456447478573,\n",
       " 2.713644859928833,\n",
       " 2.712447927117634,\n",
       " 2.71125483099735,\n",
       " 2.7100655563255835,\n",
       " 2.7088800879339923,\n",
       " 2.707698410727853,\n",
       " 2.706520509685642,\n",
       " 2.7053463698585754,\n",
       " 2.7041759763702085,\n",
       " 2.7030093144159846,\n",
       " 2.7018463692628254,\n",
       " 2.700687126248709,\n",
       " 2.6995315707822445,\n",
       " 2.698379688342262,\n",
       " 2.697231464477403,\n",
       " 2.6960868848057045,\n",
       " 2.694945935014198,\n",
       " 2.6938086008585005,\n",
       " 2.6926748681624053,\n",
       " 2.6915447228175164,\n",
       " 2.6904181507828064,\n",
       " 2.6892951380842587,\n",
       " 2.688175670814463,\n",
       " 2.6870597351322236,\n",
       " 2.6859473172621873,\n",
       " 2.684838403494447,\n",
       " 2.683732980184168,\n",
       " 2.68263103375121,\n",
       " 2.6815325506797523,\n",
       " 2.6804375175179236,\n",
       " 2.6793459208774255,\n",
       " 2.6782577474331632,\n",
       " 2.6771729839229015,\n",
       " 2.6760916171468696,\n",
       " 2.6750136339674295,\n",
       " 2.673939021308703,\n",
       " 2.6728677661562195,\n",
       " 2.6717998555565647,\n",
       " 2.6707352766170294,\n",
       " 2.6696740165052546,\n",
       " 2.6686160624489026,\n",
       " 2.6675614017352927,\n",
       " 2.666510021711077,\n",
       " 2.665461909781891,\n",
       " 2.6644170534120173,\n",
       " 2.66337544012406,\n",
       " 2.6623370574985983,\n",
       " 2.661301893173872,\n",
       " 2.660269934845441,\n",
       " 2.6592411702658674,\n",
       " 2.6582155872443827,\n",
       " 2.6571931736465824,\n",
       " 2.656173917394089,\n",
       " 2.6551578064642474,\n",
       " 2.6541448288898,\n",
       " 2.6531349727585876,\n",
       " 2.652128226213226,\n",
       " 2.6511245774507954,\n",
       " 2.650124014722547,\n",
       " 2.6491265263335886]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 5 (3 балла)**. Обучите и провалидируйте модели на данных из предыдущего пункта, сравните качество между методами по метрикам MSE и $R^2$. Исследуйте влияние параметров `max_iter` и `eta` (`max_iter`, `alpha` и `eta` для Momentum) на процесс оптимизации. Согласуется ли оно с вашими ожиданиями?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 6 (2 балла)**. Постройте графики (на одной и той же картинке) зависимости величины функции потерь от номера итерации для полного, стохастического градиентного спусков, а также для полного градиентного спуска с методом Momentum. Сделайте выводы о скорости сходимости различных модификаций градиентного спуска.\n",
    "\n",
    "Не забывайте о том, что должны получиться *красивые* графики!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бонус "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 7 (2 балла)**. Реализуйте линейную регрессию с функцией потерь MSE, обучаемую с помощью метода\n",
    "[Adam](https://arxiv.org/pdf/1412.6980.pdf) - добавьте при необходимости параметры в класс модели, повторите пункты 5 и 6 и сравните результаты. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 8 (2 балла)**. Реализуйте линейную регрессию с функцией потерь\n",
    "$$ L(\\hat{y}, y) = log(cosh(\\hat{y} - y)),$$\n",
    "\n",
    "обучаемую с помощью градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 9 (0.01 балла)**.  Вставьте картинку с вашим любимым мемом в этот Jupyter Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
