{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение, ФКН ВШЭ\n",
    "\n",
    "## Практическое задание 3. Градиентный спуск своими руками\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: 05.10.2019\n",
    "\n",
    "Мягкий дедлайн: 07:59MSK 15.10.2019 (за каждый день просрочки снимается 1 балл)\n",
    "\n",
    "Жесткий дедлайн: 23:59MSK 17.10.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска.\n",
    "\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "\n",
    "### Формат сдачи\n",
    "Задания сдаются через систему Anytask. Инвайт можно найти на странице курса. Присылать необходимо ноутбук с выполненным заданием. \n",
    "\n",
    "Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.\n",
    "\n",
    "**Оценка**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация градиентного спуска\n",
    "\n",
    "Реализуйте линейную регрессию с функцией потерь MSE, обучаемую с помощью:\n",
    "\n",
    "** Задание 1 (1 балл)** Градиентного спуска;\n",
    "\n",
    "** Задание 2 (1.5 балла)** Стохастического градиентного спуска;\n",
    "\n",
    "** Задание 3 (2.5 балла)** Метода Momentum.\n",
    "\n",
    "\n",
    "Во всех пунктах необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы;\n",
    "* Циклы средствами python допускается использовать только для итераций градиентного спуска;\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "\n",
    "    * проверку на евклидовую норму разности весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$, задаваемого параметром `tolerance`);\n",
    "    * достижение максимального числа итераций (например, 10000, задаваемого параметром `max_iter`).\n",
    "* Чтобы проследить, что оптимизационный процесс действительно сходится, будем использовать атрибут класса `loss_history` — в нём после вызова метода `fit` должны содержаться значения функции потерь для всех итераций, начиная с первой (до совершения первого шага по антиградиенту);\n",
    "* Инициализировать веса можно случайным образом или нулевым вектором. \n",
    "\n",
    "\n",
    "Ниже приведён шаблон класса, который должен содержать код реализации каждого из методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearReg(BaseEstimator):\n",
    "    def __init__(self, gd_type='stochastic', \n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, alpha=1e-3, eta=1e-2):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic' or 'momentum'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        if self.w0 == None:\n",
    "            self.w0 = np.zeros(X.shape[1])\n",
    "        self.w = self.w0\n",
    "        self.loss_history.append(self.calc_loss(X, y))\n",
    "        for i in range(self.max_iter):\n",
    "            #gradient step and new weights\n",
    "            grad_step = self.eta*self.calc_gradient(X, y)\n",
    "            self.w = self.w - grad_step\n",
    "            #append loss history\n",
    "            self.loss_history.append(self.calc_loss(X, y))\n",
    "            #checking tolerance for weights\n",
    "            if (grad_step**2).sum() < self.tolerance:\n",
    "                break\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return X.dot(self.w)\n",
    "        pass\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d) (ell can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        if self.gd_type=='full':\n",
    "            A = X.dot(self.w) - y\n",
    "            grad = X.T.dot(A)*2/X.shape[0]\n",
    "        return grad\n",
    "        pass\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\" \n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return ((X.dot(self.w) - y)**2).sum()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 4 (0 баллов)**. \n",
    "* Загрузите данные из домашнего задания 2 ([train.csv](https://www.kaggle.com/c/nyc-taxi-trip-duration/data));\n",
    "* Разбейте выборку на обучающую и тестовую в отношении 7:3 с random_seed=0;\n",
    "* Преобразуйте целевую переменную `trip_duration` как $\\hat{y} = \\log{(y + 1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "import pandas as pd\n",
    "import math\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train['log_trip_duration'] = np.log(df['trip_duration'] + 1)\n",
    "\n",
    "#Data cleaning\n",
    "df_train.drop('id',inplace=True,axis=1)\n",
    "df_train.drop('dropoff_datetime',inplace=True,axis=1)\n",
    "df_train[\"pickup_datetime\"]=pd.to_datetime(df_train[\"pickup_datetime\"])\n",
    "df_train['day_of_week'] = df_train['pickup_datetime'].dt.day_name()\n",
    "df_train['hour_of_the_day']=df_train['pickup_datetime'].dt.hour\n",
    "df_train['month']=df_train['pickup_datetime'].dt.month\n",
    "df_train['day_of_week']=df_train['day_of_week'].map({'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6,'Sunday':7})\n",
    "df_train['store_and_fwd_flag']=df_train['store_and_fwd_flag'].map({'N':0,'Y':1})\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # distance between latitudes and longitudes\n",
    "    dLat = (lat2 - lat1) * math.pi / 180.0\n",
    "    dLon = (lon2 - lon1) * math.pi / 180.0\n",
    " \n",
    "    # convert to radians\n",
    "    lat1 = (lat1) * math.pi / 180.0\n",
    "    lat2 = (lat2) * math.pi / 180.0\n",
    " \n",
    "    # apply formulae\n",
    "    a = (pow(math.sin(dLat / 2), 2) +\n",
    "         pow(math.sin(dLon / 2), 2) *\n",
    "             math.cos(lat1) * math.cos(lat2));\n",
    "    rad = 6371\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return rad * c\n",
    "df_train['distance']=df_train.apply(lambda row:haversine(row['pickup_latitude'],row['pickup_longitude'],row['dropoff_latitude'],row['dropoff_longitude']),axis=1)\n",
    "df_train['distance']=df_train['distance'].astype(float)\n",
    "df_train.drop('pickup_longitude',inplace=True,axis=1)\n",
    "df_train.drop('pickup_latitude',inplace=True,axis=1)\n",
    "df_train.drop('dropoff_longitude',inplace=True,axis=1)\n",
    "df_train.drop('dropoff_latitude',inplace=True,axis=1)\n",
    "df_train.drop('pickup_datetime',inplace=True,axis=1)\n",
    "\n",
    "#train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train.drop(columns = ['log_trip_duration', 'trip_duration']),\n",
    "                                                            df_train['log_trip_duration'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571578    NaN\n",
       "1280332   NaN\n",
       "177838    NaN\n",
       "1433776   NaN\n",
       "757662    NaN\n",
       "           ..\n",
       "1042944   NaN\n",
       "85930     NaN\n",
       "61268     NaN\n",
       "251164    NaN\n",
       "1348190   NaN\n",
       "Length: 437594, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearReg(gd_type = 'full')\n",
    "reg.fit(X_train, y_train)\n",
    "reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43349090.9815841,\n",
       " 1499534114.929395,\n",
       " 59344623524.46554,\n",
       " 2359911925402.5156,\n",
       " 93860873820154.1,\n",
       " 3733156715429854.5,\n",
       " 1.4848000210604195e+17,\n",
       " 5.905541329900544e+18,\n",
       " 2.3488293320287283e+20,\n",
       " 9.342072001412777e+21,\n",
       " 3.715651371072303e+23,\n",
       " 1.4778375834894367e+25,\n",
       " 5.8778494133684635e+26,\n",
       " 2.337815339941438e+28,\n",
       " 9.29826570792228e+29,\n",
       " 3.6982281576306676e+31,\n",
       " 1.4709077945836075e+33,\n",
       " 5.850287348287727e+34,\n",
       " 2.326852994019532e+36,\n",
       " 9.25466482832221e+37,\n",
       " 3.680886644094755e+39,\n",
       " 1.4640105004355388e+41,\n",
       " 5.822854525618304e+42,\n",
       " 2.315942052084106e+44,\n",
       " 9.2112683993973e+45,\n",
       " 3.66362644736216e+47,\n",
       " 1.4571455486727216e+49,\n",
       " 5.795550339324308e+50,\n",
       " 2.3050822730946072e+52,\n",
       " 9.1680754624495e+53,\n",
       " 3.646447186126917e+55,\n",
       " 1.4503127876371496e+57,\n",
       " 5.768374186211597e+58,\n",
       " 2.294273417140768e+60,\n",
       " 9.12508506327625e+61,\n",
       " 3.62934848087107e+63,\n",
       " 1.4435120663819696e+65,\n",
       " 5.741325465914576e+66,\n",
       " 2.2835152454372945e+68,\n",
       " 9.08229625214933e+69,\n",
       " 3.612329953856261e+71,\n",
       " 1.4367432346681237e+73,\n",
       " 5.714403580882747e+74,\n",
       " 2.272807520318579e+76,\n",
       " 9.03970808379395e+77,\n",
       " 3.595391229115411e+79,\n",
       " 1.4300061429610504e+81,\n",
       " 5.6876079363676455e+82,\n",
       " 2.2621500052334653e+84,\n",
       " 8.997319617367825e+85,\n",
       " 3.578531932444392e+87,\n",
       " 1.42330064242739e+89,\n",
       " 5.66093794040974e+90,\n",
       " 2.2515424647400338e+92,\n",
       " 8.955129916440488e+93,\n",
       " 3.5617516913937876e+95,\n",
       " 1.416626584931665e+97,\n",
       " 5.634393003825148e+98,\n",
       " 2.240984664500365e+100,\n",
       " 8.913138048972444e+101,\n",
       " 3.5450501352606385e+103,\n",
       " 1.4099838230330445e+105,\n",
       " 5.607972540192904e+106,\n",
       " 2.230476371275401e+108,\n",
       " 8.871343087294707e+109,\n",
       " 3.5284268950802657e+111,\n",
       " 1.403372209982051e+113,\n",
       " 5.58167596584172e+114,\n",
       " 2.2200173529197652e+116,\n",
       " 8.829744108088261e+117,\n",
       " 3.511881603618125e+119,\n",
       " 1.396791599717348e+121,\n",
       " 5.555502699837301e+122,\n",
       " 2.2096073783766364e+124,\n",
       " 8.788340192363618e+125,\n",
       " 3.495413895361676e+127,\n",
       " 1.390241846862494e+129,\n",
       " 5.529452163969405e+130,\n",
       " 2.1992462176726683e+132,\n",
       " 8.747130425440629e+133,\n",
       " 3.479023406512323e+135,\n",
       " 1.3837228067227416e+137,\n",
       " 5.503523782739132e+138,\n",
       " 2.1889336419128746e+140,\n",
       " 8.706113896928139e+141,\n",
       " 3.462709774977364e+143,\n",
       " 1.3772343352818383e+145,\n",
       " 5.477716983346102e+146,\n",
       " 2.1786694232755975e+148,\n",
       " 8.665289700703976e+149,\n",
       " 3.446472640362017e+151,\n",
       " 1.370776289198843e+153,\n",
       " 5.452031195675982e+154,\n",
       " 2.1684533350074756e+156,\n",
       " 8.624656934894958e+157,\n",
       " 3.4303116439614453e+159,\n",
       " 1.3643485258049602e+161,\n",
       " 5.426465852287703e+162,\n",
       " 2.1582851514184163e+164,\n",
       " 8.584214701856823e+165,\n",
       " 3.4142264287527976e+167,\n",
       " 1.3579509031003868e+169,\n",
       " 5.401020388401044e+170,\n",
       " 2.1481646478766178e+172,\n",
       " 8.543962108154555e+173,\n",
       " 3.398216639387389e+175,\n",
       " 1.3515832797511811e+177,\n",
       " 5.375694241884118e+178,\n",
       " 2.1380916008036175e+180,\n",
       " 8.503898264542549e+181,\n",
       " 3.382281922182806e+183,\n",
       " 1.345245515086134e+185,\n",
       " 5.350486853240932e+186,\n",
       " 2.1280657876693287e+188,\n",
       " 8.464022285944965e+189,\n",
       " 3.366421925115067e+191,\n",
       " 1.3389374690936551e+193,\n",
       " 5.32539766559905e+194,\n",
       " 2.1180869869871513e+196,\n",
       " 8.424333291436266e+197,\n",
       " 3.350636297810935e+199,\n",
       " 1.3326590024187077e+201,\n",
       " 5.300426124697342e+202,\n",
       " 2.1081549783090768e+204,\n",
       " 8.384830404221735e+205,\n",
       " 3.334924691540097e+207,\n",
       " 1.326409976359696e+209,\n",
       " 5.275571678873634e+210,\n",
       " 2.098269542220792e+212,\n",
       " 8.345512751617979e+213,\n",
       " 3.319286759207481e+215,\n",
       " 1.3201902528654202e+217,\n",
       " 5.250833779052574e+218,\n",
       " 2.0884304603368543e+220,\n",
       " 8.306379465033781e+221,\n",
       " 3.303722155345608e+223,\n",
       " 1.3139996945320177e+225,\n",
       " 5.226211878733543e+226,\n",
       " 2.0786375152958604e+228,\n",
       " 8.267429679950876e+229,\n",
       " 3.288230536106922e+231,\n",
       " 1.3078381645999378e+233,\n",
       " 5.2017054339784894e+234,\n",
       " 2.0688904907556493e+236,\n",
       " 8.228662535904842e+237,\n",
       " 3.272811559256235e+239,\n",
       " 1.3017055269509123e+241,\n",
       " 5.177313903399984e+242,\n",
       " 2.0591891713885018e+244,\n",
       " 8.190077176466068e+245,\n",
       " 3.2574648841631366e+247,\n",
       " 1.295601646104943e+249,\n",
       " 5.153036748149254e+250,\n",
       " 2.049533342876422e+252,\n",
       " 8.151672749220898e+253,\n",
       " 3.242190171794491e+255,\n",
       " 1.2895263872173293e+257,\n",
       " 5.128873431904236e+258,\n",
       " 2.0399227919063743e+260,\n",
       " 8.113448405752729e+261,\n",
       " 3.226987084706921e+263,\n",
       " 1.2834796160756692e+265,\n",
       " 5.1048234208577815e+266,\n",
       " 2.030357306165569e+268,\n",
       " 8.075403301623337e+269,\n",
       " 3.2118552870393777e+271,\n",
       " 1.277461199096897e+273,\n",
       " 5.08088618370581e+274,\n",
       " 2.0208366743367872e+276,\n",
       " 8.03753659635418e+277,\n",
       " 3.196794444505719e+279,\n",
       " 1.271471003324351e+281,\n",
       " 5.057061191635651e+282,\n",
       " 2.0113606860937203e+284,\n",
       " 7.999847453407827e+285,\n",
       " 3.181804224387316e+287,\n",
       " 1.2655088964248122e+289,\n",
       " 5.033347918314267e+290,\n",
       " 2.0019291320962897e+292,\n",
       " 7.962335040169532e+293,\n",
       " 3.16688429552571e+295,\n",
       " 1.2595747466855945e+297,\n",
       " 5.009745839876707e+298,\n",
       " 1.9925418039860565e+300,\n",
       " 7.924998527928759e+301,\n",
       " 3.152034328315281e+303,\n",
       " 1.25366842301163e+305,\n",
       " 4.986254434914461e+306,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " 0.0]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 5 (3 балла)**. Обучите и провалидируйте модели на данных из предыдущего пункта, сравните качество между методами по метрикам MSE и $R^2$. Исследуйте влияние параметров `max_iter` и `eta` (`max_iter`, `alpha` и `eta` для Momentum) на процесс оптимизации. Согласуется ли оно с вашими ожиданиями?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 6 (2 балла)**. Постройте графики (на одной и той же картинке) зависимости величины функции потерь от номера итерации для полного, стохастического градиентного спусков, а также для полного градиентного спуска с методом Momentum. Сделайте выводы о скорости сходимости различных модификаций градиентного спуска.\n",
    "\n",
    "Не забывайте о том, что должны получиться *красивые* графики!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бонус "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 7 (2 балла)**. Реализуйте линейную регрессию с функцией потерь MSE, обучаемую с помощью метода\n",
    "[Adam](https://arxiv.org/pdf/1412.6980.pdf) - добавьте при необходимости параметры в класс модели, повторите пункты 5 и 6 и сравните результаты. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 8 (2 балла)**. Реализуйте линейную регрессию с функцией потерь\n",
    "$$ L(\\hat{y}, y) = log(cosh(\\hat{y} - y)),$$\n",
    "\n",
    "обучаемую с помощью градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 9 (0.01 балла)**.  Вставьте картинку с вашим любимым мемом в этот Jupyter Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
