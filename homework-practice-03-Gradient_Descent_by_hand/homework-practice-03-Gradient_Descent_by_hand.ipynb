{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение, ФКН ВШЭ\n",
    "\n",
    "## Практическое задание 3. Градиентный спуск своими руками\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: 05.10.2019\n",
    "\n",
    "Мягкий дедлайн: 07:59MSK 15.10.2019 (за каждый день просрочки снимается 1 балл)\n",
    "\n",
    "Жесткий дедлайн: 23:59MSK 17.10.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска.\n",
    "\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "\n",
    "### Формат сдачи\n",
    "Задания сдаются через систему Anytask. Инвайт можно найти на странице курса. Присылать необходимо ноутбук с выполненным заданием. \n",
    "\n",
    "Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.\n",
    "\n",
    "**Оценка**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация градиентного спуска\n",
    "\n",
    "Реализуйте линейную регрессию с функцией потерь MSE, обучаемую с помощью:\n",
    "\n",
    "** Задание 1 (1 балл)** Градиентного спуска;\n",
    "\n",
    "** Задание 2 (1.5 балла)** Стохастического градиентного спуска;\n",
    "\n",
    "** Задание 3 (2.5 балла)** Метода Momentum.\n",
    "\n",
    "\n",
    "Во всех пунктах необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы;\n",
    "* Циклы средствами python допускается использовать только для итераций градиентного спуска;\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "\n",
    "    * проверку на евклидовую норму разности весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$, задаваемого параметром `tolerance`);\n",
    "    * достижение максимального числа итераций (например, 10000, задаваемого параметром `max_iter`).\n",
    "* Чтобы проследить, что оптимизационный процесс действительно сходится, будем использовать атрибут класса `loss_history` — в нём после вызова метода `fit` должны содержаться значения функции потерь для всех итераций, начиная с первой (до совершения первого шага по антиградиенту);\n",
    "* Инициализировать веса можно случайным образом или нулевым вектором. \n",
    "\n",
    "\n",
    "Ниже приведён шаблон класса, который должен содержать код реализации каждого из методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearReg(BaseEstimator):\n",
    "    def __init__(self, gd_type='stochastic', \n",
    "                 tolerance=1e-4,max_iter=1000, w0=None, alpha=1e-3, eta=1e-2):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic' or 'momentum'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        if self.w0 == None:\n",
    "            self.w0 = np.zeros(X.shape[1])\n",
    "        self.w = self.w0\n",
    "        self.loss_history.append(self.calc_loss(X, y))\n",
    "        if self.gd_type == 'full':\n",
    "            for i in range(self.max_iter):\n",
    "                #gradient step and new weights\n",
    "                grad_step = self.eta*self.calc_gradient(X, y)\n",
    "                self.w = self.w - grad_step\n",
    "                #append loss history\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                #checking tolerance for weights\n",
    "                #if (grad_step**2).sum() < self.tolerance:\n",
    "                #    break\n",
    "        elif self.gd_type == 'stochastic':\n",
    "            for i in range(self.max_iter):\n",
    "                #gradient step and new weights\n",
    "                batch_size = 18\n",
    "                rows = np.random.randint(X.shape[0], size=batch_size)\n",
    "                X_batch, y_batch = X.iloc[rows, :], y.iloc[rows]\n",
    "                grad_step = self.eta*self.calc_gradient(X_batch, y_batch)\n",
    "                self.w = self.w - grad_step\n",
    "                #append loss history\n",
    "                self.loss_history.append(self.calc_loss(X_batch, y_batch))\n",
    "                #checking tolerance for weights\n",
    "                #if (grad_step**2).sum() < self.tolerance:\n",
    "                #    break\n",
    "        elif self.gd_type == 'momentum':\n",
    "            for i in range(self.max_iter):\n",
    "                #gradient step and new weights\n",
    "                if i == 0:\n",
    "                    grad_step = np.zeros(X.shape[1])\n",
    "                batch_size = 18\n",
    "                rows = np.random.randint(X.shape[0], size=batch_size)\n",
    "                X_batch, y_batch = X.iloc[rows, :], y.iloc[rows]\n",
    "                grad_step = self.eta*self.calc_gradient(X_batch, y_batch) - self.alpha*grad_step\n",
    "                self.w = self.w - grad_step\n",
    "                #append loss history\n",
    "                self.loss_history.append(self.calc_loss(X_batch, y_batch))\n",
    "                #checking tolerance for weights\n",
    "                if (grad_step**2).sum() < self.tolerance:\n",
    "                    break\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return X.dot(self.w)\n",
    "        pass\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d) (ell can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        A = X.dot(self.w) - y\n",
    "        grad = X.T.dot(A)*2/X.shape[0]\n",
    "        return grad\n",
    "        pass\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\" \n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return ((X.dot(self.w) - y)**2).sum()/X.shape[0]\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 4 (0 баллов)**. \n",
    "* Загрузите данные из домашнего задания 2 ([train.csv](https://www.kaggle.com/c/nyc-taxi-trip-duration/data));\n",
    "* Разбейте выборку на обучающую и тестовую в отношении 7:3 с random_seed=0;\n",
    "* Преобразуйте целевую переменную `trip_duration` как $\\hat{y} = \\log{(y + 1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "import pandas as pd\n",
    "import math\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train['log_trip_duration'] = np.log(df_train['trip_duration'] + 1)\n",
    "\n",
    "#Data cleaning\n",
    "df_train.drop('id',inplace=True,axis=1)\n",
    "df_train.drop('dropoff_datetime',inplace=True,axis=1)\n",
    "df_train[\"pickup_datetime\"]=pd.to_datetime(df_train[\"pickup_datetime\"])\n",
    "df_train['day_of_week'] = df_train['pickup_datetime'].dt.day_name()\n",
    "df_train['hour_of_the_day']=df_train['pickup_datetime'].dt.hour\n",
    "df_train['month']=df_train['pickup_datetime'].dt.month\n",
    "df_train['day_of_week']=df_train['day_of_week'].map({'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6,'Sunday':7})\n",
    "df_train['store_and_fwd_flag']=df_train['store_and_fwd_flag'].map({'N':0,'Y':1})\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # distance between latitudes and longitudes\n",
    "    dLat = (lat2 - lat1) * math.pi / 180.0\n",
    "    dLon = (lon2 - lon1) * math.pi / 180.0\n",
    " \n",
    "    # convert to radians\n",
    "    lat1 = (lat1) * math.pi / 180.0\n",
    "    lat2 = (lat2) * math.pi / 180.0\n",
    " \n",
    "    # apply formulae\n",
    "    a = (pow(math.sin(dLat / 2), 2) +\n",
    "         pow(math.sin(dLon / 2), 2) *\n",
    "             math.cos(lat1) * math.cos(lat2));\n",
    "    rad = 6371\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return rad * c\n",
    "df_train['distance']=df_train.apply(lambda row:haversine(row['pickup_latitude'],row['pickup_longitude'],row['dropoff_latitude'],row['dropoff_longitude']),axis=1)\n",
    "df_train['distance']=df_train['distance'].astype(float)\n",
    "df_train.drop('pickup_longitude',inplace=True,axis=1)\n",
    "df_train.drop('pickup_latitude',inplace=True,axis=1)\n",
    "df_train.drop('dropoff_longitude',inplace=True,axis=1)\n",
    "df_train.drop('dropoff_latitude',inplace=True,axis=1)\n",
    "df_train.drop('pickup_datetime',inplace=True,axis=1)\n",
    "\n",
    "#train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train.drop(columns = ['log_trip_duration', 'trip_duration']),\n",
    "                                                            df_train['log_trip_duration'], test_size=0.3, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571578     5.948872\n",
       "1280332    9.622025\n",
       "177838     6.081259\n",
       "1433776    6.916753\n",
       "757662     4.994833\n",
       "             ...   \n",
       "1042944    9.234280\n",
       "85930      3.493622\n",
       "61268      9.215319\n",
       "251164     7.481971\n",
       "1348190    5.114252\n",
       "Length: 437594, dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full\n",
    "eta = 1e-3\n",
    "tolerance = 1e-4\n",
    "full = LinearReg(gd_type = 'full', eta = eta, tolerance = tolerance)\n",
    "full.fit(X_train, y_train)\n",
    "full.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.968411772062771,\n",
       " 1.9682174491876483,\n",
       " 1.9680233743762063,\n",
       " 1.9678295472995062,\n",
       " 1.967635967629139,\n",
       " 1.9674426350372218,\n",
       " 1.9672495491963942,\n",
       " 1.9670567097798182,\n",
       " 1.9668641164611802,\n",
       " 1.9666717689146813]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(full.loss_history))\n",
    "full.loss_history[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9216342073382396e-07\n",
      "vendor_id            -0.427706\n",
      "passenger_count       0.073363\n",
      "store_and_fwd_flag   -0.007613\n",
      "day_of_week           0.040954\n",
      "hour_of_the_day       0.011839\n",
      "month                 0.043982\n",
      "distance              0.006249\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#A = X_train_scaled.dot(np.zeros(X_train_scaled.shape[1])) - y_train\n",
    "A = X_train.dot(full.w) - y_train\n",
    "grad = X_train.T.dot(A)*2/X_train.shape[0]\n",
    "print(((eta * grad)**2).sum())\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00953011,  0.00958804,  0.00072774, -0.02234542,  0.03676376,\n",
       "        0.02983414,  0.46565774])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571578     6.145147\n",
       "1280332    9.836212\n",
       "177838     5.782384\n",
       "1433776    6.269399\n",
       "757662     4.666073\n",
       "             ...   \n",
       "1042944    8.469792\n",
       "85930      3.467316\n",
       "61268      8.395616\n",
       "251164     7.404607\n",
       "1348190    4.943981\n",
       "Length: 437594, dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stochastic\n",
    "eta = 1e-3\n",
    "tolerance = 1e-4\n",
    "stochastic = LinearReg(gd_type = 'stochastic', eta = eta, tolerance = tolerance)\n",
    "stochastic.fit(X_train, y_train)\n",
    "stochastic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[42.45540471238833,\n",
       " 14.121016848293065,\n",
       " 5.176440711987471,\n",
       " 7.485111955899037,\n",
       " 6.115403663443519,\n",
       " 5.416700431168257,\n",
       " 6.84012710136543,\n",
       " 4.456616333681877,\n",
       " 3.5021660878582797,\n",
       " 2.5857579264421933,\n",
       " 3.853294635246286,\n",
       " 2.8675544132319297,\n",
       " 4.030398952386527,\n",
       " 2.8847244568230344,\n",
       " 4.73828803107622,\n",
       " 1.7129322773500801,\n",
       " 5.345381475180989,\n",
       " 3.1987520623639836,\n",
       " 4.527434650948686,\n",
       " 3.715630238872247,\n",
       " 2.9395775396812827,\n",
       " 3.4382172678422154,\n",
       " 3.027925093924151,\n",
       " 3.418505666251199,\n",
       " 3.361348100198037,\n",
       " 1.4368697236981012,\n",
       " 1.8486153059660042,\n",
       " 3.221071362650058,\n",
       " 1.8223364326784015,\n",
       " 3.380610753746462,\n",
       " 2.983379633293504,\n",
       " 3.936815199973923,\n",
       " 2.452973796032663,\n",
       " 3.9238834319370812,\n",
       " 2.771881339612851,\n",
       " 3.1971012191835064,\n",
       " 3.0008819040892054,\n",
       " 2.1104463284107804,\n",
       " 5.09168576789915,\n",
       " 2.9822386179410207,\n",
       " 2.7691929390885326,\n",
       " 1.6187506919735886,\n",
       " 3.301886552784549,\n",
       " 2.1067042885642886,\n",
       " 4.045425367095115,\n",
       " 2.555781099045287,\n",
       " 2.74137923057997,\n",
       " 3.0196051878123145,\n",
       " 3.4204387613276435,\n",
       " 2.3192141114119327,\n",
       " 2.213648817504107,\n",
       " 1.6932528318912818,\n",
       " 3.596305476635786,\n",
       " 1.2730268244063216,\n",
       " 0.7665576217571299,\n",
       " 4.729662918025469,\n",
       " 4.512842630142158,\n",
       " 1.594770062856746,\n",
       " 2.230395832452496,\n",
       " 2.683458716152662,\n",
       " 3.0087092678109335,\n",
       " 2.8823333312661155,\n",
       " 2.3520840927673747,\n",
       " 1.1411198346027411,\n",
       " 1.5416683651789742,\n",
       " 2.7306193674908332,\n",
       " 2.013237081453118,\n",
       " 5.084793502181956,\n",
       " 1.7743856877585271,\n",
       " 2.109175641231532,\n",
       " 1.9898464200807493,\n",
       " 1.9063788033214888,\n",
       " 2.717293997646829,\n",
       " 1.678242848320859,\n",
       " 3.1635138043732822,\n",
       " 3.579414515343419,\n",
       " 2.22690156108032,\n",
       " 2.082438314101508,\n",
       " 1.007186912259446,\n",
       " 2.451413622466072,\n",
       " 1.86459130136625,\n",
       " 2.3907497392883506,\n",
       " 4.106151121866955,\n",
       " 2.558093916168997,\n",
       " 1.6736240633979869,\n",
       " 1.2566575946556726,\n",
       " 1.777166791803894,\n",
       " 1.5654402136828185,\n",
       " 1.7692196597572198,\n",
       " 2.275002991419134,\n",
       " 1.7025937450377358,\n",
       " 2.262364697947387,\n",
       " 0.9091698584090623,\n",
       " 2.9125148859360803,\n",
       " 1.5871187042610526,\n",
       " 2.638116057908373,\n",
       " 3.2198324598977783,\n",
       " 2.7852179029491793,\n",
       " 2.7634947402073835,\n",
       " 3.3940091641255115,\n",
       " 2.45909248406837,\n",
       " 2.716609286195413,\n",
       " 1.5827412377622307,\n",
       " 2.225450658327848,\n",
       " 2.2396414214921765,\n",
       " 1.7701527806224506,\n",
       " 2.4850602670170425,\n",
       " 2.6837345790149674,\n",
       " 1.6076934285479867,\n",
       " 2.0370256660963637,\n",
       " 2.268617355337753,\n",
       " 0.9392877496689961,\n",
       " 2.0573873249409065,\n",
       " 3.7578407788197765,\n",
       " 1.066257465154953,\n",
       " 3.399146562438,\n",
       " 1.6017310451025368,\n",
       " 1.7395715988377214,\n",
       " 1.8269297060529708,\n",
       " 2.7398883183034575,\n",
       " 2.8475826208618384,\n",
       " 2.1607116857488453,\n",
       " 1.2782654361004573,\n",
       " 2.866658599969132,\n",
       " 2.355836922960448,\n",
       " 2.4493033722778166,\n",
       " 0.837118928218197,\n",
       " 2.845700085504506,\n",
       " 1.7316223894189426,\n",
       " 1.1647857966844846,\n",
       " 1.5520586913443208,\n",
       " 2.8753610120776543,\n",
       " 2.4792355063977607,\n",
       " 1.8977622859767278,\n",
       " 2.2705749449125037,\n",
       " 1.4315614763065552,\n",
       " 1.4190546959831891,\n",
       " 2.7300127470728297,\n",
       " 2.1131239524933423,\n",
       " 2.2749659186774887,\n",
       " 2.227672462215961,\n",
       " 1.9066400881303869,\n",
       " 1.509659198135139,\n",
       " 1.7015198931935065,\n",
       " 1.1993002158728603,\n",
       " 1.5266170315373815,\n",
       " 2.712042524741157,\n",
       " 1.9145552350069994,\n",
       " 3.850653125211385,\n",
       " 2.2068238379428284,\n",
       " 1.4865774005432604,\n",
       " 1.9128125450600284,\n",
       " 1.9093927555299548,\n",
       " 1.448850336170186,\n",
       " 2.226632227518794,\n",
       " 2.8440102921558816,\n",
       " 2.743880495671632,\n",
       " 2.6739235810874726,\n",
       " 1.2117311716287518,\n",
       " 2.0891955419881416,\n",
       " 2.0831103315163344,\n",
       " 0.5917491460181163,\n",
       " 2.1873567067929622,\n",
       " 1.558714127631479,\n",
       " 1.4466164186604797,\n",
       " 2.0934440265104155,\n",
       " 1.1448942862896172,\n",
       " 2.970602265127151,\n",
       " 2.1261092007344384,\n",
       " 1.7501681253699264,\n",
       " 2.2355774311779117,\n",
       " 2.100735663528409,\n",
       " 0.8757132561411483,\n",
       " 1.4771828860244964,\n",
       " 2.0346781661948423,\n",
       " 1.1133765319673454,\n",
       " 1.172250172736442,\n",
       " 1.850525371511167,\n",
       " 2.943332823137375,\n",
       " 2.0307439078010394,\n",
       " 1.3961409987189053,\n",
       " 1.6599314365923632,\n",
       " 2.8060308547646615,\n",
       " 1.84748478438488,\n",
       " 2.023266520656852,\n",
       " 1.4640797535349073,\n",
       " 1.705188037456485,\n",
       " 3.5609722993716324,\n",
       " 4.447634204686064,\n",
       " 2.016412283180988,\n",
       " 3.4774076252169563,\n",
       " 1.6297125357153939,\n",
       " 1.781622753744544,\n",
       " 2.084352400229065,\n",
       " 1.7009635951924296,\n",
       " 1.8165263427248364,\n",
       " 2.7246072045785734,\n",
       " 1.6659448512919282,\n",
       " 2.6298095263454297,\n",
       " 2.8213765723131154,\n",
       " 2.6046182499104344,\n",
       " 2.4678067213376433,\n",
       " 1.7907087123192795,\n",
       " 3.0529758014772557,\n",
       " 2.773040162763025,\n",
       " 2.3926621219812843,\n",
       " 2.146550283418835,\n",
       " 2.7071419548663096,\n",
       " 1.8570394888186672,\n",
       " 2.6371424867007986,\n",
       " 2.3946224810941525,\n",
       " 2.088730644323851,\n",
       " 2.017955169555119,\n",
       " 2.408420179516221,\n",
       " 2.2915363921830423,\n",
       " 1.115109668459978,\n",
       " 1.5821338554968118,\n",
       " 0.98663463367561,\n",
       " 3.152609283836452,\n",
       " 2.8195971742696155,\n",
       " 2.040292093701419,\n",
       " 2.0318459459898266,\n",
       " 2.4082493956533084,\n",
       " 1.0016426227441784,\n",
       " 1.9583120014498923,\n",
       " 2.5926000592013274,\n",
       " 2.318620775765314,\n",
       " 1.4200708858626545,\n",
       " 2.2307975867301097,\n",
       " 2.425583574953208,\n",
       " 1.8756840024112447,\n",
       " 0.9213171709878544,\n",
       " 1.8731627539551856,\n",
       " 1.8079767655743406,\n",
       " 3.2656632622230015,\n",
       " 2.9462440883437324,\n",
       " 2.4987122630198275,\n",
       " 1.9010209285509907,\n",
       " 1.4599867365405867,\n",
       " 1.1916872023670244,\n",
       " 1.6540517854872312,\n",
       " 3.1964628799578945,\n",
       " 1.9630904495015065,\n",
       " 2.3344426264775895,\n",
       " 1.5932739664141835,\n",
       " 1.8276622096952333,\n",
       " 2.623161106860149,\n",
       " 1.791711107844699,\n",
       " 2.543670099683028,\n",
       " 4.670290656420807,\n",
       " 1.7384355525393633,\n",
       " 2.357949553729848,\n",
       " 3.0872017133598124,\n",
       " 1.030719651102713,\n",
       " 2.8235680421937146,\n",
       " 2.2084520386950315,\n",
       " 1.4356253559872938,\n",
       " 1.7665461773530422,\n",
       " 1.8906311987125792,\n",
       " 3.08660335913469,\n",
       " 1.9940350861625125,\n",
       " 1.982948510198793,\n",
       " 1.3647038828904736,\n",
       " 1.9100527274345878,\n",
       " 1.27984662783215,\n",
       " 1.4895217923344914,\n",
       " 1.3999517241849464,\n",
       " 2.3458760972420407,\n",
       " 2.210161142977032,\n",
       " 2.1149114122839814,\n",
       " 2.0626524311921712,\n",
       " 1.8148157063062282,\n",
       " 1.290354571114056,\n",
       " 2.86046378296016,\n",
       " 1.7467073796549482,\n",
       " 2.591847112518293,\n",
       " 3.2567170078452827,\n",
       " 1.5429983383772377,\n",
       " 4.284697220385166,\n",
       " 1.913064834262831,\n",
       " 2.293512019913069,\n",
       " 2.3535585688904557,\n",
       " 1.6671802533452018,\n",
       " 1.0357743464659022,\n",
       " 2.068103103170335,\n",
       " 1.499400639478542,\n",
       " 1.501794419704879,\n",
       " 2.4671966898360407,\n",
       " 2.179482980482031,\n",
       " 3.305967544562033,\n",
       " 2.077664132295455,\n",
       " 1.4750795998441304,\n",
       " 1.8148541639145914,\n",
       " 3.5592529664236445,\n",
       " 1.5799826476308254,\n",
       " 2.3527164950341293,\n",
       " 2.2396737648648233,\n",
       " 2.8271900037224267,\n",
       " 2.9755176317726697,\n",
       " 1.6265818822188483,\n",
       " 0.7266777934775726,\n",
       " 2.3395188307691743,\n",
       " 2.6038377092618403,\n",
       " 1.1293911134392907,\n",
       " 2.7172248629547893,\n",
       " 2.0508341113566906,\n",
       " 2.6095036218683996,\n",
       " 1.2680011803874172,\n",
       " 3.0943621010161726,\n",
       " 1.4900718672271838,\n",
       " 1.9044692712595785,\n",
       " 1.6657255981136236,\n",
       " 1.604593598196196,\n",
       " 2.881453998782904,\n",
       " 1.5738871552276472,\n",
       " 2.1250671214232852,\n",
       " 1.3167030528141677,\n",
       " 0.9771931166952754,\n",
       " 2.243219261844935,\n",
       " 2.0057405838677838,\n",
       " 2.1272858756804442,\n",
       " 1.5499493445325865,\n",
       " 2.5197700588889322,\n",
       " 1.4530056758907977,\n",
       " 1.5442256705308022,\n",
       " 1.6663550169140235,\n",
       " 1.9672338324160397,\n",
       " 1.503817620553357,\n",
       " 2.01213018986967,\n",
       " 2.1639953970219508,\n",
       " 1.515189841368076,\n",
       " 2.4617576768010716,\n",
       " 1.7303673543219262,\n",
       " 1.0155877730528498,\n",
       " 1.1480226702947283,\n",
       " 2.0118639472075652,\n",
       " 2.5766261915624913,\n",
       " 1.4572821404800957,\n",
       " 1.3325381838980264,\n",
       " 1.9169320229876072,\n",
       " 3.5741779839995087,\n",
       " 2.0621514858913557,\n",
       " 2.3134617912860924,\n",
       " 1.9669689347446335,\n",
       " 1.6765318682872694,\n",
       " 2.1966371811010488,\n",
       " 1.6012748001317751,\n",
       " 2.9054150999044395,\n",
       " 0.9957739548834204,\n",
       " 1.8332324032246694,\n",
       " 2.1484689291426635,\n",
       " 0.6310790613307437,\n",
       " 1.9638682984261417,\n",
       " 1.7559826651319044,\n",
       " 2.315358735918145,\n",
       " 3.5632599704472927,\n",
       " 2.2618779215150893,\n",
       " 0.9599524382786281,\n",
       " 2.0875877007114854,\n",
       " 1.96653283634376,\n",
       " 2.7895328906061594,\n",
       " 1.5268825348701422,\n",
       " 2.5949486846776697,\n",
       " 1.693760892632561,\n",
       " 1.340783661168543,\n",
       " 1.4955268210881427,\n",
       " 1.3501514072505403,\n",
       " 4160.870433481006,\n",
       " 43.59863068125544,\n",
       " 21.50432250159946,\n",
       " 31.466963855952194,\n",
       " 23.892597409793854,\n",
       " 4.833043436715576,\n",
       " 15.839216757667778,\n",
       " 15.27142111204738,\n",
       " 8.318039935487814,\n",
       " 13.335925015917823,\n",
       " 9.82309021737914,\n",
       " 7.4051592023395205,\n",
       " 15.282644576953171,\n",
       " 13.231568910516948,\n",
       " 10.816771338476604,\n",
       " 19.224217386027437,\n",
       " 16.13970795528754,\n",
       " 21.319900690203824,\n",
       " 11.327127777938742,\n",
       " 11.777567864610049,\n",
       " 5.177468055017373,\n",
       " 4.946217188124297,\n",
       " 6.683662664379249,\n",
       " 4.777439804205425,\n",
       " 6.70116767443005,\n",
       " 6.124603943887953,\n",
       " 4.725789422919039,\n",
       " 3.182164019529106,\n",
       " 5.777769044470798,\n",
       " 3.5702689784508865,\n",
       " 3.325111311420635,\n",
       " 2.662908230374842,\n",
       " 4.579692625456813,\n",
       " 2.475643333836344,\n",
       " 3.927586689702218,\n",
       " 2.7117868131062375,\n",
       " 5.016001863558299,\n",
       " 3.656151740776107,\n",
       " 2.5355582511460226,\n",
       " 3.7078279938712515,\n",
       " 2.803380135700567,\n",
       " 4.968325837434911,\n",
       " 1.9422436847585398,\n",
       " 1.4463654767699046,\n",
       " 2.7350078668098154,\n",
       " 2.903523656753973,\n",
       " 4.640476873987048,\n",
       " 2.398500138194796,\n",
       " 2.0264463232654863,\n",
       " 2.415052182797837,\n",
       " 2.7278812657956646,\n",
       " 2.8445729689468227,\n",
       " 2.9623891744638207,\n",
       " 2.476123182470429,\n",
       " 1.8135012675560849,\n",
       " 1.531809070382229,\n",
       " 2.1152873279312665,\n",
       " 3.890634415224783,\n",
       " 2.076543041143956,\n",
       " 2.416036610305706,\n",
       " 2.624744477886206,\n",
       " 1.4667013880765483,\n",
       " 3.7380698504618497,\n",
       " 3.1777761292626305,\n",
       " 2.5707174568899758,\n",
       " 2.1970817027908187,\n",
       " 1.9210374357640214,\n",
       " 1.3027560582151156,\n",
       " 0.8225129946306603,\n",
       " 1.8775159095979785,\n",
       " 2.3250208766397935,\n",
       " 1.4002193068789497,\n",
       " 2.457832889183751,\n",
       " 2.390476032839697,\n",
       " 1.448951403816269,\n",
       " 2.301679832706681,\n",
       " 2.468547011579208,\n",
       " 2.968437421567944,\n",
       " 0.9605003999896753,\n",
       " 1.5241936669644962,\n",
       " 1.7628644112587797,\n",
       " 1.3456234718058735,\n",
       " 1.9995800397490255,\n",
       " 1.7407555236246435,\n",
       " 2.594746027760483,\n",
       " 2.019041678768835,\n",
       " 1.9050361199191106,\n",
       " 1.190581214736607,\n",
       " 2.3608657696746636,\n",
       " 1.970053887372339,\n",
       " 2.676757603394113,\n",
       " 1.305066830723176,\n",
       " 1.5433351207047725,\n",
       " 1.6692379785336717,\n",
       " 2.1339417719349543,\n",
       " 1.4910607939370442,\n",
       " 3.0297282650096533,\n",
       " 1.577151160186376,\n",
       " 1.317715531557425,\n",
       " 2.135936655297402,\n",
       " 1.7869580654066424,\n",
       " 1.0175971172795277,\n",
       " 2.761412728357156,\n",
       " 1.0507905644415358,\n",
       " 2.010095719049019,\n",
       " 2.651097377472795,\n",
       " 3.593531332737557,\n",
       " 1.4802217737321883,\n",
       " 1.7016151946875073,\n",
       " 1.1890829876515714,\n",
       " 2.190170638252636,\n",
       " 3.2428899386901757,\n",
       " 2.1952624219268935,\n",
       " 0.9835659798353308,\n",
       " 2.7401472239111437,\n",
       " 1.3073643219631104,\n",
       " 1.5218911485171869,\n",
       " 1.7344066984883502,\n",
       " 1.3387591964136227,\n",
       " 3.6458451786412587,\n",
       " 2.199743097775946,\n",
       " 1.6545262736808708,\n",
       " 3.744210373794316,\n",
       " 2.0634823161811164,\n",
       " 1.1155371012544577,\n",
       " 0.9966265457205795,\n",
       " 1.7914881229977564,\n",
       " 1.5439618787491183,\n",
       " 1.137546320944976,\n",
       " 3.3332817886289057,\n",
       " 2.0342308653853522,\n",
       " 0.9009832247085853,\n",
       " 1.8596597359523268,\n",
       " 1.4400385223022643,\n",
       " 1.5478243920094163,\n",
       " 1.816461661927028,\n",
       " 1.6596439424759692,\n",
       " 1.8534654534097976,\n",
       " 2.386819181948353,\n",
       " 0.7558672949250833,\n",
       " 2.6499889279398876,\n",
       " 0.977514065177597,\n",
       " 1.7491532690919607,\n",
       " 1.594673171352118,\n",
       " 3.134845263976521,\n",
       " 1.1081056024498905,\n",
       " 1.5886107140963235,\n",
       " 1.7230229949586662,\n",
       " 3.2022204197694353,\n",
       " 1.866549820736575,\n",
       " 1.452197737844497,\n",
       " 1.1614685255212618,\n",
       " 1.6322995842407175,\n",
       " 1.8793624480511066,\n",
       " 1.9892555048524068,\n",
       " 2.7014404285415163,\n",
       " 1.1654111077445177,\n",
       " 1.037862253770457,\n",
       " 1.5778938486580667,\n",
       " 1.2135585281390382,\n",
       " 1.5700659353961184,\n",
       " 1.811867551043678,\n",
       " 1.903227503845664,\n",
       " 1.9219316229063093,\n",
       " 2.3230949879357636,\n",
       " 1.6895076046330497,\n",
       " 1.7060141326973437,\n",
       " 1.8048967186287872,\n",
       " 0.7134869262132889,\n",
       " 1.8350857513326626,\n",
       " 1.138940255546769,\n",
       " 2.250285316918669,\n",
       " 1.3539562554599747,\n",
       " 2.4551983713969223,\n",
       " 1.9611700161347154,\n",
       " 1.9677806362689978,\n",
       " 2.8015404072735652,\n",
       " 1.682246935866528,\n",
       " 1.9172329103845804,\n",
       " 1.8979643513232491,\n",
       " 1.6658902124700008,\n",
       " 1.8112117029674546,\n",
       " 2.3253268805617258,\n",
       " 2.7999165862218693,\n",
       " 1.3431571533763342,\n",
       " 1.0142065909741054,\n",
       " 1.5078636620868762,\n",
       " 1.946877339832231,\n",
       " 2.1588955737839424,\n",
       " 3.659771370260206,\n",
       " 1.1548228692471598,\n",
       " 1.8390192848143094,\n",
       " 0.8741217648777961,\n",
       " 2.636232404922303,\n",
       " 1.643819371334523,\n",
       " 1.6332772357393408,\n",
       " 2.024254374837695,\n",
       " 1.9352660191535351,\n",
       " 2.1112120716401317,\n",
       " 1.8970299356497655,\n",
       " 1.8591649126363832,\n",
       " 2.27259276669957,\n",
       " 1.2403172618772942,\n",
       " 1.947724715693492,\n",
       " 1.6537845186566678,\n",
       " 1.8866741111273457,\n",
       " 1.1847116210980075,\n",
       " 2.440925838799565,\n",
       " 1.4321410482170949,\n",
       " 1.2511948433519842,\n",
       " 1.4445944895007115,\n",
       " 1.6266778262366037,\n",
       " 2.3735351915684633,\n",
       " 1.3082789234067949,\n",
       " 1.3360863984523275,\n",
       " 1.2754279829249462,\n",
       " 1.4460799963222977,\n",
       " 1.8035388541555681,\n",
       " 1.4278381499149109,\n",
       " 2.0590637379055217,\n",
       " 1.0054837910359824,\n",
       " 2.2053976661877615,\n",
       " 2.553419546980189,\n",
       " 1.216097514579495,\n",
       " 2.3388710226762517,\n",
       " 3.7292549559692296,\n",
       " 1.2015516136421,\n",
       " 0.6946312385029144,\n",
       " 2.4823858540934913,\n",
       " 1.7151330399804616,\n",
       " 3.8799241343049156,\n",
       " 1.294982458846311,\n",
       " 1.239180033167723,\n",
       " 1.7468609560707404,\n",
       " 0.5149342496867749,\n",
       " 2.8864093489520966,\n",
       " 1.9515062482428769,\n",
       " 1.8351663640747162,\n",
       " 2.3054287251784498,\n",
       " 2.266186761185672,\n",
       " 2.041018962259982,\n",
       " 2.822146243882265,\n",
       " 2.8749155691974146,\n",
       " 2.8299520816320407,\n",
       " 1.3390755677285924,\n",
       " 1.7051848636147457,\n",
       " 2.6294163225021223,\n",
       " 2.5238630612508586,\n",
       " 2.214188863462381,\n",
       " 1.2713970068089397,\n",
       " 2.0353522407154774,\n",
       " 1.5220340181039387,\n",
       " 2.3788847735318397,\n",
       " 1.9743294509354081,\n",
       " 1.882342750779275,\n",
       " 0.728362726244066,\n",
       " 2.479364496954367,\n",
       " 4.108171719121943,\n",
       " 2.395535618719288,\n",
       " 1.3670886558642088,\n",
       " 3.142927612530892,\n",
       " 3.6372420677359765,\n",
       " 3.0635273947583133,\n",
       " 1.4286906999600022,\n",
       " 2.051419648719084,\n",
       " 2.098613317128166,\n",
       " 1.3009386488040233,\n",
       " 2.068979134267922,\n",
       " 1.6835390608337815,\n",
       " 2.287277437075211,\n",
       " 1.1799237737015136,\n",
       " 2.2191739208158983,\n",
       " 1.039589290871107,\n",
       " 1.5316326497145611,\n",
       " 1.1054365874158363,\n",
       " 1.388358382503904,\n",
       " 1.1876338067468437,\n",
       " 1.271151654255617,\n",
       " 1.3513853644647247,\n",
       " 2.0537943753090584,\n",
       " 2.240724547463744,\n",
       " 1.9674055844922738,\n",
       " 2.914640693162386,\n",
       " 2.306874511976673,\n",
       " 1.565090259342983,\n",
       " 0.9009063394619936,\n",
       " 2.1136177149083535,\n",
       " 3.0484958479682613,\n",
       " 1.873424105915039,\n",
       " 3.4275784498858255,\n",
       " 1.9754045603267079,\n",
       " 1.8363003841395817,\n",
       " 1.1431833150693853,\n",
       " 1.5904831763371727,\n",
       " 1.451894748481828,\n",
       " 2.5454439756515246,\n",
       " 1.9094194663772623,\n",
       " 1.6676526279823713,\n",
       " 2.4941927281990863,\n",
       " 1.681936978170409,\n",
       " 1.3570447460055401,\n",
       " 2.869878742538829,\n",
       " 2.486150306197052,\n",
       " 2.3235345147508024,\n",
       " 2.94685005894001,\n",
       " 3.21127171995604,\n",
       " 2.5200265571242784,\n",
       " 2.1194348306956035,\n",
       " 1.053359447618797,\n",
       " 1.6943752599885704,\n",
       " 1.770199178058507,\n",
       " 1.07758877187174,\n",
       " 3.2522895188277388,\n",
       " 2.688236890904118,\n",
       " 2.0591795890809155,\n",
       " 2.380181519205252,\n",
       " 1.5199275158676504,\n",
       " 1.528641089227515,\n",
       " 0.9150664629739794,\n",
       " 1.9775056243036613,\n",
       " 1.4788455252210668,\n",
       " 1.974106836217754,\n",
       " 1.5465001524904318,\n",
       " 2.249283551965531,\n",
       " 2.231947315039495,\n",
       " 1.6517630654759428,\n",
       " 2.3650805199538576,\n",
       " 2.1397830344409243,\n",
       " 1.9822793701802783,\n",
       " 2.50730007399253,\n",
       " 1.7419096077400782,\n",
       " 2.149691279266363,\n",
       " 2.832206320511274,\n",
       " 1.906931949846059,\n",
       " 2.007611368127039,\n",
       " 1.1159755030812424,\n",
       " 1.775043803054781,\n",
       " 1.930966800652595,\n",
       " 1.5434013044057653,\n",
       " 4.038900634554299,\n",
       " 3.093124221234403,\n",
       " 1.5466502966547286,\n",
       " 1.965107287948636,\n",
       " 0.8802126319258876,\n",
       " 1.8863744022416131,\n",
       " 2.7615336317273496,\n",
       " 1.2670562101882514,\n",
       " 2.430809772615118,\n",
       " 2.494609835121234,\n",
       " 1.0628562569807742,\n",
       " 1.1080589951048043,\n",
       " 1.8800482987407297,\n",
       " 2.0819550868078975,\n",
       " 2.368335443366136,\n",
       " 1.9918672265074748,\n",
       " 1.2766013810485224,\n",
       " 0.6522891056133894,\n",
       " 1.9741223144711826,\n",
       " 1.5061014638022767,\n",
       " 2.003416749629714,\n",
       " 1.2643707589747573,\n",
       " 2.428583211732721,\n",
       " 1.4476150480232102,\n",
       " 1.8570100030469447,\n",
       " 0.7903149754596648,\n",
       " 2.1393576302180204,\n",
       " 1.391903701067613,\n",
       " 2.5966436194604197,\n",
       " 1.7757821604913042,\n",
       " 1.02585676590165,\n",
       " 2.7863743992393775,\n",
       " 1.6307189333982965,\n",
       " 1.9492089808104727,\n",
       " 2.4953678145716562,\n",
       " 1.348298546123365,\n",
       " 2.393252701127124,\n",
       " 2.0144599845452467,\n",
       " 1.2354247575198087,\n",
       " 3.801282282684643,\n",
       " 3.113338736960078,\n",
       " 1.3324961943305311,\n",
       " 1.8668384853718583,\n",
       " 2.207431085119326,\n",
       " 1.7589728531318183,\n",
       " 3.2979157564621575,\n",
       " 1.1032784592906864,\n",
       " 2.1277669857078316,\n",
       " 2.556532295545774,\n",
       " 0.9165977462126899,\n",
       " 2.175385793435865,\n",
       " 2.4453357489573118,\n",
       " 2.871477681712936,\n",
       " 2.0003911383430024,\n",
       " 2.432048033917745,\n",
       " 0.9660297958990964,\n",
       " 1.6492132692665473,\n",
       " 2.014901293404049,\n",
       " 2.5993349676630833,\n",
       " 1.4662434293223305,\n",
       " 2.7907904365402123,\n",
       " 2.6645678656053158,\n",
       " 0.8345585250843583,\n",
       " 1.2613084083743307,\n",
       " 1.86160259604857,\n",
       " 1.7526198160223259,\n",
       " 1.591662469807863,\n",
       " 1.6504931707133237,\n",
       " 1.6553886208164943,\n",
       " 1.0004786957500702,\n",
       " 0.9798054468715015,\n",
       " 1.2151900965325013,\n",
       " 2.6224912262059767,\n",
       " 2.1802142135684948,\n",
       " 1.0817489076789002,\n",
       " 3.2235188611133907,\n",
       " 3.038053162701683,\n",
       " 2.227913499604318,\n",
       " 1.7029483825232277,\n",
       " 1.0984859637225601,\n",
       " 1.7715290962381607,\n",
       " 1.909569523274754,\n",
       " 1.3575753146719851,\n",
       " 1.9002089528420016,\n",
       " 1.8675402981375515,\n",
       " 1.2733661768464097,\n",
       " 2.0111052685826154,\n",
       " 1.1400410788626392,\n",
       " 1.265250644739472,\n",
       " 0.6981387448521585,\n",
       " 1.907658800793656,\n",
       " 2.4159498978761027,\n",
       " 1.401745990692877,\n",
       " 3.2069443351528264,\n",
       " 1.7449921078033772,\n",
       " 2.957837654892144,\n",
       " 1.9671518228053937,\n",
       " 1.9481604653518514,\n",
       " 2.017115546617247,\n",
       " 2.4812022712244057,\n",
       " 2.200911826362545,\n",
       " 2.44472890917349,\n",
       " 1.3602405264822348,\n",
       " 1.3085677872592443,\n",
       " 0.7587669657679825,\n",
       " 1.6025973910318707,\n",
       " 2.0202703918992495,\n",
       " 1.2912425907311806,\n",
       " 2.216345513659233,\n",
       " 1.673951925764072,\n",
       " 2.691337996374827,\n",
       " 3.242332077827247,\n",
       " 1.987834608828068,\n",
       " 2.1632367801228463,\n",
       " 2.1183358708695197,\n",
       " 1.023939984385074,\n",
       " 1.7056884117130915,\n",
       " 3.1646386313826023,\n",
       " 1.6780982530010697,\n",
       " 2.053510480646495,\n",
       " 1.3214861753053866,\n",
       " 1.3433698506594707,\n",
       " 1.328545376715174,\n",
       " 1.039478222580326,\n",
       " 2.4579851242657664,\n",
       " 1.7024868909688253,\n",
       " 2.5241411917040453,\n",
       " 1.7507066471832404,\n",
       " 0.9915910349868177,\n",
       " 2.1339693823108146,\n",
       " 3.629636622780197,\n",
       " 1.5082914949789772,\n",
       " 2.243740198203315,\n",
       " 2.1447453299510157,\n",
       " 1.1405916084749765,\n",
       " 2.384387963185484,\n",
       " 2.8748687754028217,\n",
       " 2.558169151820966,\n",
       " 2.1516690925319746,\n",
       " 1.9779083735385328,\n",
       " 1.7246317652380136,\n",
       " 1.0853541388586898,\n",
       " 1.7141726741167693,\n",
       " 1.3454368522956666,\n",
       " 2.5277498060067143,\n",
       " 2.9042024085197395,\n",
       " 1.5328961155029237,\n",
       " 1.896586595718662,\n",
       " 1.5131309449401278,\n",
       " 2.3294839759597505,\n",
       " 2.2308994286614707,\n",
       " 1.3086353707561214,\n",
       " 2.262138646922402,\n",
       " 1.645559690800314,\n",
       " 1.4812776515031816,\n",
       " 2.7516226804318316,\n",
       " 1.1290943837405936,\n",
       " 2.670746795745062,\n",
       " 3.1084889502658086,\n",
       " 1.6879729322878014,\n",
       " 2.002366035599481,\n",
       " 1.0939510168253914,\n",
       " 2.648245261828739,\n",
       " 1.068456071501133,\n",
       " 1.483546215195923,\n",
       " 2.0304283759867525,\n",
       " 0.8923785260263141,\n",
       " 1.5308808753087944,\n",
       " 1.4159456985226182,\n",
       " 1.5111117058325665,\n",
       " 0.807870477136021,\n",
       " 2.36572529699129,\n",
       " 1.2399739820806128,\n",
       " 1.7700411479547737,\n",
       " 1.2462073233843183,\n",
       " 1.8760981062399675,\n",
       " 2.786226463938019,\n",
       " 1.6594082299179007,\n",
       " 1.2229571194720135,\n",
       " 1.0630426867375105,\n",
       " 0.994254176351611,\n",
       " 2.7275921362364226,\n",
       " 1.465885531632174,\n",
       " 2.1811625890349635,\n",
       " 1.81152257907419,\n",
       " 1.5735326681752566,\n",
       " 2.0049770042826744,\n",
       " 2.070613191245429,\n",
       " 1.648724773156245,\n",
       " 1.1206628156211067,\n",
       " 1.5098614411326485,\n",
       " 1.2179957515739555,\n",
       " 0.892163530962371,\n",
       " 1.8600625316399477,\n",
       " 2.4599266490513783,\n",
       " 2.1915467355619325,\n",
       " 568686.7842357712,\n",
       " 300.4737823617105,\n",
       " 1140.490138721318,\n",
       " 170.50425009247542,\n",
       " 154.2668652552794,\n",
       " 151.32585090076972,\n",
       " 107.46961908367916,\n",
       " 231.6195519743126,\n",
       " 144.8763279590313,\n",
       " 310.9218460707323,\n",
       " 467.01643920380815,\n",
       " 345.78282892026266,\n",
       " 505.2939748220376,\n",
       " 99.92406562024448,\n",
       " 31.461182125808005,\n",
       " 154.56193173406007,\n",
       " 140.27033644205707,\n",
       " 93.36477065540761,\n",
       " 53.06323081807048,\n",
       " 128.57834632859718,\n",
       " 147.4487427726266,\n",
       " 63.66691232791072,\n",
       " 88.97348869923442,\n",
       " 291.0279927652113,\n",
       " 92.02248171018404,\n",
       " 173.93351034434983,\n",
       " 48.93512014184594,\n",
       " 33.23775090126422,\n",
       " 71.1825300826482,\n",
       " 46.829763303156334,\n",
       " 81.34597377939708,\n",
       " 152.79732408973385,\n",
       " 165.84333152770571,\n",
       " 52.46185529539793,\n",
       " 36.777977828901854,\n",
       " 99.05057193176135,\n",
       " 71.51411595883195,\n",
       " 14.943698097124987,\n",
       " 13.921777832534781,\n",
       " 43.97359273261966,\n",
       " 92.71844512007033,\n",
       " 21.170892093682404,\n",
       " 10.469818570757797,\n",
       " 52.93500704624981,\n",
       " 24.542438403293986,\n",
       " 53.73321251976785,\n",
       " 40.91934227948514,\n",
       " 20.129303474362576,\n",
       " 18.12039983328082,\n",
       " 29.680502537779667,\n",
       " 56.04612055148746,\n",
       " 24.09040328860734,\n",
       " 25.611119904384676,\n",
       " 54.661347905480746,\n",
       " 33.83521527607398,\n",
       " 47.131836999972656,\n",
       " 16.3510712215148,\n",
       " 21.947908792598994,\n",
       " 12.058098334614366,\n",
       " 17.697569239763343,\n",
       " 14.17567958305904,\n",
       " 21.944434345818255,\n",
       " 20.875947712453918,\n",
       " 11.134386484767619,\n",
       " 9.598890745316375,\n",
       " 5.834888480289343,\n",
       " 9.100701099002212,\n",
       " 15.20396982600013,\n",
       " 5.1348323917102245,\n",
       " 8.650019138275074,\n",
       " 24.524200521358225,\n",
       " 24.094837748221195,\n",
       " 13.125460783478772,\n",
       " 20.959453303341707,\n",
       " 10.888633294290983,\n",
       " 11.079640885878144,\n",
       " 15.400431988793754,\n",
       " 6.962820499182132,\n",
       " 11.052119489853458,\n",
       " 10.451989899129215,\n",
       " 17.878750671547337,\n",
       " 4.298354818948098,\n",
       " 5.736275484940932,\n",
       " 13.88035699801242,\n",
       " 5.681532912956002,\n",
       " 10.22112809610947,\n",
       " 3.9385755800450832,\n",
       " 9.919582427955172,\n",
       " 3.715353737439962,\n",
       " 3.861794838840816,\n",
       " 2.822948435467206,\n",
       " 6.755546060871868,\n",
       " 4.107522289427724,\n",
       " 4.484047213192885,\n",
       " 2.815682196784407,\n",
       " 2.5234136247501144,\n",
       " 6.648338086392872,\n",
       " 3.693857513061962,\n",
       " ...]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(stochastic.loss_history))\n",
    "stochastic.loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.086584181684913e-05\n",
      "vendor_id            -0.419244\n",
      "passenger_count      -0.451769\n",
      "store_and_fwd_flag   -0.010615\n",
      "day_of_week          -0.968443\n",
      "hour_of_the_day      -4.319850\n",
      "month                -0.885766\n",
      "distance              0.319850\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#A = X_train_scaled.dot(np.zeros(X_train_scaled.shape[1])) - y_train\n",
    "A = X_train.dot(stochastic.w) - y_train\n",
    "grad = X_train.T.dot(A)*2/X_train.shape[0]\n",
    "print(((eta * grad)**2).sum())\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571578     5.921512\n",
       "1280332    9.732845\n",
       "177838     5.534182\n",
       "1433776    7.767580\n",
       "757662     5.028692\n",
       "             ...   \n",
       "1042944    8.288165\n",
       "85930      1.614175\n",
       "61268      7.090202\n",
       "251164     3.124340\n",
       "1348190    2.122740\n",
       "Length: 437594, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#momentum\n",
    "momentum = LinearReg(gd_type = 'momentum')\n",
    "momentum.fit(X_train, y_train)\n",
    "momentum.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.241047225198867,\n",
       " 4.623833826805121,\n",
       " 6.017158393746772,\n",
       " 1.6657281832375983,\n",
       " 4.490639019304258,\n",
       " 4.2409132645658225,\n",
       " 6.270801272876176,\n",
       " 6.777180844754564,\n",
       " 4.152997971091964,\n",
       " 3.85376439224361]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(momentum.loss_history))\n",
    "momentum.loss_history[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8101986615022594"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression(fit_intercept = False).fit(X_train, y_train)\n",
    "((reg.predict(X_train) - y_train)**2).sum()/X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 5 (3 балла)**. Обучите и провалидируйте модели на данных из предыдущего пункта, сравните качество между методами по метрикам MSE и $R^2$. Исследуйте влияние параметров `max_iter` и `eta` (`max_iter`, `alpha` и `eta` для Momentum) на процесс оптимизации. Согласуется ли оно с вашими ожиданиями?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full descent MSE: 4.529793425362567, r2: -6.194165170788836\n",
      "stochastic descent MSE: 4.16717617822024, r2: -5.618260681389332\n",
      "momentum descent MSE: 4.366896372653161, r2: -5.935453968537409\n"
     ]
    }
   ],
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print(f'full descent MSE: {mean_squared_error(y_test, full.predict(X_test))}, r2: {r2_score(y_test, full.predict(X_test))}')\n",
    "\n",
    "print(f'stochastic descent MSE: {mean_squared_error(y_test, stochastic.predict(X_test))}, r2: {r2_score(y_test, stochastic.predict(X_test))}')\n",
    "\n",
    "print(f'momentum descent MSE: {mean_squared_error(y_test, momentum.predict(X_test))}, r2: {r2_score(y_test, momentum.predict(X_test))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full 500 0.01 0.3\n",
      "not_converged\n",
      "full 500 0.01 0.5\n",
      "not_converged\n",
      "full 500 0.01 0.7\n",
      "not_converged\n",
      "full 500 0.005 0.3\n",
      "MSE: 8.270516726158389e+219, r2: -1.3135138358101358e+220\n",
      "full 500 0.005 0.5\n",
      "MSE: 8.270516726158389e+219, r2: -1.3135138358101358e+220\n",
      "full 500 0.005 0.7\n",
      "MSE: 8.270516726158389e+219, r2: -1.3135138358101358e+220\n",
      "full 500 0.001 0.3\n",
      "MSE: 4.529793425362567, r2: -6.194165170788836\n",
      "full 500 0.001 0.5\n",
      "MSE: 4.529793425362567, r2: -6.194165170788836\n",
      "full 500 0.001 0.7\n",
      "MSE: 4.529793425362567, r2: -6.194165170788836\n",
      "full 500 0.0001 0.3\n",
      "MSE: 13.504186325951563, r2: -20.447191472804867\n",
      "full 500 0.0001 0.5\n",
      "MSE: 13.504186325951563, r2: -20.447191472804867\n",
      "full 500 0.0001 0.7\n",
      "MSE: 13.504186325951563, r2: -20.447191472804867\n",
      "full 500 1e-05 0.3\n",
      "MSE: 42.06401554566813, r2: -65.80557967341397\n",
      "full 500 1e-05 0.5\n",
      "MSE: 42.06401554566813, r2: -65.80557967341397\n",
      "full 500 1e-05 0.7\n",
      "MSE: 42.06401554566813, r2: -65.80557967341397\n",
      "full 1000 0.01 0.3\n",
      "not_converged\n",
      "full 1000 0.01 0.5\n",
      "not_converged\n",
      "full 1000 0.01 0.7\n",
      "not_converged\n",
      "full 1000 0.005 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_regression.py:254: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_regression.py:591: RuntimeWarning: overflow encountered in square\n",
      "  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: inf, r2: -inf\n",
      "full 1000 0.005 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_regression.py:254: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_regression.py:591: RuntimeWarning: overflow encountered in square\n",
      "  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: inf, r2: -inf\n",
      "full 1000 0.005 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_regression.py:254: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_regression.py:591: RuntimeWarning: overflow encountered in square\n",
      "  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: inf, r2: -inf\n",
      "full 1000 0.001 0.3\n",
      "MSE: 4.529793425362567, r2: -6.194165170788836\n",
      "full 1000 0.001 0.5\n",
      "MSE: 4.529793425362567, r2: -6.194165170788836\n",
      "full 1000 0.001 0.7\n",
      "MSE: 4.529793425362567, r2: -6.194165170788836\n",
      "full 1000 0.0001 0.3\n",
      "MSE: 13.504186325951563, r2: -20.447191472804867\n",
      "full 1000 0.0001 0.5\n",
      "MSE: 13.504186325951563, r2: -20.447191472804867\n",
      "full 1000 0.0001 0.7\n",
      "MSE: 13.504186325951563, r2: -20.447191472804867\n",
      "full 1000 1e-05 0.3\n",
      "MSE: 42.06401554566813, r2: -65.80557967341397\n",
      "full 1000 1e-05 0.5\n",
      "MSE: 42.06401554566813, r2: -65.80557967341397\n",
      "full 1000 1e-05 0.7\n",
      "MSE: 42.06401554566813, r2: -65.80557967341397\n",
      "full 5000 0.01 0.3\n",
      "not_converged\n",
      "full 5000 0.01 0.5\n",
      "not_converged\n",
      "full 5000 0.01 0.7\n",
      "not_converged\n",
      "full 5000 0.005 0.3\n",
      "not_converged\n",
      "full 5000 0.005 0.5\n",
      "not_converged\n",
      "full 5000 0.005 0.7\n",
      "not_converged\n",
      "full 5000 0.001 0.3\n",
      "MSE: 4.529793425362567, r2: -6.194165170788836\n",
      "full 5000 0.001 0.5\n",
      "MSE: 4.529793425362567, r2: -6.194165170788836\n",
      "full 5000 0.001 0.7\n",
      "MSE: 4.529793425362567, r2: -6.194165170788836\n",
      "full 5000 0.0001 0.3\n",
      "MSE: 13.504186325951563, r2: -20.447191472804867\n",
      "full 5000 0.0001 0.5\n",
      "MSE: 13.504186325951563, r2: -20.447191472804867\n",
      "full 5000 0.0001 0.7\n",
      "MSE: 13.504186325951563, r2: -20.447191472804867\n",
      "full 5000 1e-05 0.3\n",
      "MSE: 42.06401554566813, r2: -65.80557967341397\n",
      "full 5000 1e-05 0.5\n",
      "MSE: 42.06401554566813, r2: -65.80557967341397\n",
      "full 5000 1e-05 0.7\n",
      "MSE: 42.06401554566813, r2: -65.80557967341397\n",
      "full 10000 0.01 0.3\n",
      "not_converged\n",
      "full 10000 0.01 0.5\n",
      "not_converged\n",
      "full 10000 0.01 0.7\n",
      "not_converged\n",
      "full 10000 0.005 0.3\n",
      "not_converged\n",
      "full 10000 0.005 0.5\n",
      "not_converged\n",
      "full 10000 0.005 0.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-d79962b2e8bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                 \u001b[1;31m#momentum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearReg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgd_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                 \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'not_converged'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-3703c6c006b9>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgrad_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;31m#append loss history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                 \u001b[1;31m#checking tolerance for weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgrad_step\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-3703c6c006b9>\u001b[0m in \u001b[0;36mcalc_loss\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \"\"\" \n\u001b[0;32m    105\u001b[0m         \u001b[1;31m#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1246\u001b[0m             \u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m             \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m             \u001b[0mlvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1249\u001b[0m             \u001b[0mrvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mvalues\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5671\u001b[0m         \"\"\"\n\u001b[0;32m   5672\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5673\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5675\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mas_array\u001b[1;34m(self, transpose, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m             \u001b[1;31m# The underlying data was copied within _interleave\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                 \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m             \u001b[0mitemmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for method in ['full', 'stochastic', 'momentum']:\n",
    "    for max_iter in (500, 1000, 5000, 10000):\n",
    "        for eta in (1e-2, 5e-3, 1e-3, 1e-4, 1e-5):\n",
    "            for alpha in (0.3, 0.5, 0.7):\n",
    "                print(method, max_iter, eta, alpha)\n",
    "                #momentum\n",
    "                res = LinearReg(gd_type = method, max_iter = max_iter, alpha = alpha, eta = eta)\n",
    "                res.fit(X_train, y_train)\n",
    "                if res.predict(X_test).isna().sum()>0:\n",
    "                    print('not_converged')\n",
    "                    continue\n",
    "                print(f'MSE: {mean_squared_error(y_test, res.predict(X_test))}, r2: {r2_score(y_test, res.predict(X_test))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-1a3da8ab23cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearReg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgd_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'full'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'MSE: {mean_squared_error(y_test, res.predict(X_test))}, r2: {r2_score(y_test, res.predict(X_test))}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \"\"\"\n\u001b[0;32m    251\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 252\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    253\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 578\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                     (type_err,\n\u001b[1;32m---> 60\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m     61\u001b[0m             )\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "res = LinearReg(gd_type = 'full', eta = 5e-3)\n",
    "res.fit(X_train, y_train)\n",
    "print(f'MSE: {mean_squared_error(y_test, res.predict(X_test))}, r2: {r2_score(y_test, res.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta: 0.0033, MSE: 2.6757953733405127, r2: -3.249667053526494\n",
      "eta: 0.0035, MSE: 2.4917750899401203, r2: -2.9574081822619998\n"
     ]
    }
   ],
   "source": [
    "for eta in (3.3e-3, 3.5e-3):\n",
    "    res = LinearReg(gd_type = 'full', eta = eta)\n",
    "    res.fit(X_train, y_train)\n",
    "    if res.predict(X_test).isna().sum()>0:\n",
    "                    print('not_converged')\n",
    "                    continue\n",
    "    print(f'eta: {eta}, MSE: {mean_squared_error(y_test, res.predict(X_test))}, r2: {r2_score(y_test, res.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 6 (2 балла)**. Постройте графики (на одной и той же картинке) зависимости величины функции потерь от номера итерации для полного, стохастического градиентного спусков, а также для полного градиентного спуска с методом Momentum. Сделайте выводы о скорости сходимости различных модификаций градиентного спуска.\n",
    "\n",
    "Не забывайте о том, что должны получиться *красивые* графики!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22020261bc8>]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "full = LinearReg(gd_type = 'full', eta = 3.5e-3)\n",
    "full.fit(X_train, y_train)\n",
    "plt.plot(full.loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22021418208>]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "stochastic = LinearReg(gd_type = 'stochastic', eta = 3.5e-3)\n",
    "stochastic.fit(X_train, y_train)\n",
    "plt.plot(stochastic.loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x220213ae808>]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nO3deXzU9bX/8deZJZkkJEAghEDYZRVBMCLiDq61LlfrrVpb2mrp5q2tt721t7+297a9t94uttpaW7St1H0v6LVVjApeUSAsIiRsYQ0JSYCEkGUy2+f3x8xkgSyTzITwGc7z8eCRZDKZfL5R3jmc72cRYwxKKaXs5+jvASillEoMDXSllEoSGuhKKZUkNNCVUipJaKArpVSScJ3MbzZ06FAzduzYk/ktlVLKeuvWrTtkjMnp7nknNdDHjh1LUVHRyfyWSillPRHZG8vztOWilFJJQgNdKaWShAa6UkolCQ10pZRKEhroSimVJDTQlVIqSWigK6VUkrAu0I80+Hj944r+HoZSSp1yrAv0l9eX8bWn1nPM6+/voSil1CnFukCvbw4A4AuE+nkkSil1auk20EVksohsbPOnTkS+KSLZIrJcRHZE3g4+GQNu8gcBCIT0pCWllGqr20A3xmwzxpxtjDkbOAdoBF4B7gMKjTETgcLIx33O6wsHuj+oFbpSSrXV05bLAqDUGLMXuAFYEnl8CXBjIgfWGa8/HOT+oFboSinVVk8D/Vbgmcj7ucaYCoDI22GJHFhnWlouWqErpVQ7MQe6iKQA1wMv9OQbiMgiESkSkaLq6uqeju8E0UD3aaArpVQ7PanQrwHWG2MqIx9XikgeQORtVUdfZIxZbIwpMMYU5OR0uz97t7wtFbq2XJRSqq2eBPpttLZbAJYBCyPvLwSWJmpQXWkJ9FBiK/S3iit55N3ShL6mUkqdTDGdWCQi6cAVwJfbPHw/8LyI3AnsA25J/PBO1NJyCSSmQm9oDvDjV4t5rmg/AJ+fN5a0FGdCXlsppU6mmALdGNMIDDnuscOEZ72cVE2+xFXo2yuP8aW/FrHvSCNT87IoqajD6w9qoCulrGTdStHWaYvxB/pj7+2i+lgzz35pLp+fNwZo/ReAUkrZxsJAjy4sir/l0tAcZPhAD+eNH4LHHa7KNdCVUrayLtCbEjjLpTkQxOMKB3lLoPs00JVSdrIq0I0xLYGeiJaL1x/C4w7/CNIige7VCl0pZSmrAt0XDGEihXkiAr05ECQ1UqFHb4RGe/RKKWUbqwLd62sN20T00Duq0LWHrpSylVWB3jZsEzFt0etvrdD1pqhSynbWBnoiKvTmQJsKPdpy0ZuiSilLWRXo3naBnpgKPVqZa8tFKWU7qwK9XcslQYGe6tIeulIqOVgV6G3bIb6EtVzCQR4Ndp2HrpSylVWBnsgK3RhDcyDUEuQOh5Dqcug8dKWUtawK9LZzxOM9JLo5EH6tVHfrRlxpKU5tuSilrGVVoLcNW18gvgq9OfLLwdM20N1OrdCVUtayNtDjnYfuDYRfK9pygXCgN+lKUaWUpawK9OhN0YwUZ9ybc3VUoXvcTr0pqpSyll2BHqnQMz3uuA+J7rBCT9GWi1LKXlYFepM/iMshpPVRhR5uuWigK6XsZF2gp7mduBwS90rRaIUeXfoffl9bLkope8UU6CIySEReFJGtIlIiIueLSLaILBeRHZG3g/t6sF5/EE+KE7fTEfdeLtHWSnRzLtCWi1LKbrFW6A8C/zDGTAFmAiXAfUChMWYiUBj5uE95/SHS3E7cTol7lktry6XtLBeHtlyUUtbqNtBFJAu4GPgTgDHGZ4ypBW4AlkSetgS4sa8GGdXkC+JxO3A5HQlsuRw3y0UDXSllqVgq9PFANfAXEdkgIo+JSAaQa4ypAIi8HdaH4wRae+hup8TdcolW6MfPQ9eWi1LKVrEEuguYDTxijJkFNNCD9oqILBKRIhEpqq6u7uUww6Lb3br7sEL3+kOE4txWQCml+kMsgV4GlBljVkc+fpFwwFeKSB5A5G1VR19sjFlsjCkwxhTk5OTENVivP0ha5KZovNMWvR1V6JFDLprj3FZAKaX6Q7eBbow5COwXkcmRhxYAxcAyYGHksYXA0j4ZYRtN/iAeV2KmLTZ3UKHrnuhKKZu5YnzevwBPiUgKsAv4AuFfBs+LyJ3APuCWvhliq6ZIhe4LhOJvuXTSQ49+H6WUsk1MgW6M2QgUdPCpBYkdTte8/vCBFMaY+LfP9QdJcTkQkZbHPJGWiy4uUkrZyKqVot620xbj3T43EMLjan/50QpdZ7oopWxkVaC3Tlt04I+zQvf6g+0OtwBtuSil7GZNoPuDIQIh07pSNO6boqF2q0QB0lL0XFGllL2sCfRoGyQtxYnLkZi9XDyu9hV6dF8XbbkopWxkTaBH2yCpbiduVwJ2W/QHST2hQteWi1LKXtYEutcXDvA0txO3I/6VouGboh330LVCV0rZyJpAj1bNaW4nLqcQMsS1RL/DCt2t0xaVUvayJtBbe+gO3M7wsP1xbKHbYYXe0nLRpf9KKftYE+jRCt3jCs9yAeK6MRrd6Kut6KpR7aErpWxkX6BHNucC4pq66PWH2i37BxAR3UJXKWUtawLd62vbQw8P2xdHoDcHQicsLIJw20V76EopG9kT6IHWQHc7wi2XeLbQbfYHT1hYFH19bbkopWxkTaA3+aJngLZtucTRQw8E2x0QHeVxO7TlopSykj2Bfty0Reh9yyUYMviDpsMK3aM9dKWUpawJdG/LTVEHKdEKvZfTFqOHW3RUoWvLRSllK6sCXQRSnI6Wm6K9bblED4jusIeuN0WVUpayJtCbfOGtc0Uk7pZLRwdER3ncTl1YpJSykj2BHtkLHWhtufSyQu/o+LkonYeulLKVNYEePX4OwOWIrhSNr4feUYWe5taWi1LKTjGdKSoie4BjQBAIGGMKRCQbeA4YC+wB/tkYU9M3w4wu1Q///nFHKuveBnqXFXqK3hRVStmpJxX6ZcaYs40x0cOi7wMKjTETgcLIx32myR9s2TzL7Yj3pmh3PXQNdKWUfeJpudwALIm8vwS4Mf7hdC56UxRouSna6wo90MUsF7cTXyAU19a8SinVH2INdAO8KSLrRGRR5LFcY0wFQOTtsL4YYJQ30Lo7Yuv2ub29Kdr5PPRoyEdnwiillC1i6qEDFxhjykVkGLBcRLbG+g0ivwAWAYwePboXQwxr8gUZOiAVoHX73EBvb4p2PQ89+v3SU2L98SilVP+LqUI3xpRH3lYBrwBzgEoRyQOIvK3q5GsXG2MKjDEFOTk5vR6ot820RXecK0W7rtD1XFGllJ26DXQRyRCRzOj7wJXAZmAZsDDytIXA0r4aJLSfh+6K84CLaIV+/BF0oOeKKqXsFUtPIRd4RUSiz3/aGPMPEVkLPC8idwL7gFv6bpjhqYbHz3Lp9Tz0Lma5tJ4rqqtFlVJ26TbQjTG7gJkdPH4YWNAXg+pIU5tDnaPz0Hu/UjTacumih64VulLKMlasFA2GDL5AqLXl4ohvL5fmQKhlo6/jaQ9dKWUrKwLd22YvdCDuAy68/iCpLgeRNlI7rS0XDXSllF3sCvRIO8TpEBwSzyyXUIf987bfo1nnoSulLGNFoEfbH5420wxdTkccLZdgu9dqSyt0pZStrAj01tOKWkM4xemIa/vcjqYsQutiI+2hK6VsY0WgR6cQprnbVugS1/a5nVXoelNUKWUrKwK99YSh1uG6nY5eLywK99A7vvTwzVLwastFKWUZKwI92s9uW6G7HUKg1/uhBztc9g8gInpQtFLKSnYEegcrO11ORxwtl8576IAGulLKSlYE+vHTFiG842I82+d2VqFD5JALXfqvlLKMVYHetkJ3Ox293j7XF+i8hw7hXxy6OZdSyjZWBHqHPXSng0AcFXpnC4ui30cDXSllGzsC3Z/YaYveQKjDjbmitIeulLKRFYHe0e6IbkccN0W7qdA9KRroSin7WBPoqS4HDkfrZlpul/R+pWg3FbrH5dCl/0op61gR6E3+YLsZLgAuh6NXs1z8wRDBkOm6h643RZVSFrLiFORrpucxZXhWu8d6O8ulqwOio7SHrpSykRWBfv6EIZw/YUi7x9xO6dX2uV0dEB0Vnoeuga6UsosVLZeOuHq5l0tMFXqKE69fFxYppewSc6CLiFNENojIa5GPs0VkuYjsiLwd3HfDPJG7l9MWY6nQ09xOfMFQr/eKUUqp/tCTCv0eoKTNx/cBhcaYiUBh5OOTxu3o3X7oratOu+6hQ3g2jFJK2SKmQBeRfOBa4LE2D98ALIm8vwS4MbFD65rb1bsKPdpySe1mHjqgM12UUlaJtUL/DfBvQNsEzTXGVABE3g5L8Ni65OrlwqKOFikdT4+hU0rZqNtAF5FPAlXGmHW9+QYiskhEikSkqLq6ujcv0aFwDz2em6JdzXIJ/1i0QldK2SSWCv0C4HoR2QM8C8wXkSeBShHJA4i8reroi40xi40xBcaYgpycnAQNO7o5Vy9aLh0cOH286Od0potSyibdBrox5nvGmHxjzFjgVuBtY8wdwDJgYeRpC4GlfTbKDkSnLRrTsyo9GtJdHXDhabkpqhW6Usoe8cxDvx+4QkR2AFdEPj5pUpzhfV16uoVuc+DEvdWPpy0XpZSNerRS1BjzLvBu5P3DwILEDyk2Lmc4dANBQxfZfIKWCr2rzbn0pqhSykL2rhSN7Lzo6+FMlx5V6DoPXSllEWsDPcUVrdB7FrrRCt3TRYWe6tJ56Eop+1gb6C5HJNB72EP3+oM4HdLSsulItHpv1kBXSlnE2kB3R26K+nrYFmkOhLqszqHtTVFtuSil7GFxoPe+Qu9q2T+0mbaoFbpSyiLWBrorUqH3dPm/1999he52OnA6ROehK6WsYm2gRyv0ngZ6c6DrA6KjPC6HtlyUUlaxONAjC4t6uJ+L1x9qmSHTFY9bzxVVStnF4kDv4wrdracWKaXsYm2gR6ct9nTHxfrmAOkp3Qd6qtuhPXSllFWsDXR3L26KhkKGHZX1nDFsQLfP9bicOg9dKWUViwM9Om0x9kAvq2mivjnAtLysbp/rcetNUaWUXawN9NZpi7G3XIorjgIwbUQsga43RZVSdrE20FN6cVO0uLwOh8Ck3Mxun+txO7WHrpSyirWB3nb73FgVV9QxIWdAjLNctOWilLKLvYHu6PlN0eLyupjaLRC+KaotF6WUTawN9OjioFh76DUNPsqPemO6IQqQqvPQlVKWsTbQoxV6rLNcSirqgNhuiEK45aLTFpVSNrE20N2RCj3W7XOLI4E+NcYKXW+KKqVs022gi4hHRNaIyEciskVE/jPyeLaILBeRHZG3g/t+uK3cPTzgoriijtysVIYOSI3p+WluJ/6gIdjD7XmVUqq/xFKhNwPzjTEzgbOBq0VkLnAfUGiMmQgURj4+aVwtm3PFWKGX18XcP4e2h1xola6UskO3gW7C6iMfuiN/DHADsCTy+BLgxj4ZYSdaD4nuvoJuDgTZWVUfc/8c9JALpZR9Yuqhi4hTRDYCVcByY8xqINcYUwEQeTus74bZ4ZhwOyWmCn1HZT2BkIm5fw7haYsA3h4ecaeUUv0lpkA3xgSNMWcD+cAcEZke6zcQkUUiUiQiRdXV1b0dZ4fcTkdM89CjN0R70nJJ1ZaLUsoyPZrlYoypBd4FrgYqRSQPIPK2qpOvWWyMKTDGFOTk5MQ53PZcDolpHnpxeR3pKU7GDMmI+bW15aKUsk0ss1xyRGRQ5P004HJgK7AMWBh52kJgaV8NsjNupyOmeejFFXVMGZ6JM9J3j0VroGvLRSllB1cMz8kDloiIk/AvgOeNMa+JyAfA8yJyJ7APuKUPx9kht9OBP9B1he4Phthy4Cg3n5Pfo9eOHiSti4uUUrboNtCNMZuAWR08fhhY0BeDipXLKfi7qdA3ldXS4Aty/vghPXrtaIXepIGulLKEtStFIXpTtOsKfdXOw4jA3F4GurZclFK2sDzQu5+2+H7pIablZTE4I6VHr60Li5RStrE60F2Orit0rz/I+r21zJvQs+oc2lToup+LUsoSVge629X1PPSiPTX4giHmnTG0x6/dsrBIWy5KKUvYHegO6XLa4vulh3A5hDljs3v82rqwSCllG6sD3eWULqctrio9zNmjBpGRGsvszPZSXQ5EdNqiUsoeVge62+nodNri0SY/H5f1rn8O4b1iUl0O3ctFKWUN6wO9s0Oi1+w+QsjQq/55lMet54oqpexheaBLpzdF3995CI/bwazRg3r9+npQtFLKJlYHuquL3RY/KD3MuWOzSY3MVukNj9uhs1yUUtawOtDdney22OQLsq3yGOf2YnZLW9pyUUrZxO5Adzo6XCl6pNEHwPAsT1yvn+p26k1RpZQ1rA50l9OBv4NDnGsawoE+KN0d1+t7XA6t0JVS1rA60FM6uSl6JBLo2T3cv+V4HrdT56ErpaxhdaC7Opm2WBNpufR0Q67j6U1RpZRNLA/0bir09PgrdN2cSyllC6sDPaWTaYs1jX4cAllp8fbQdZaLUsoeVge6y+EgZCB43I3RmgYfA9PcPTpDtCPaclFK2cTqQHe7woF9fJV+pNEXd/8cdB66Usou3Qa6iIwSkXdEpEREtojIPZHHs0VkuYjsiLwd3PfDbc/tCA8/0EGFHm//HMLz0JsDIYzp+pg7pZQ6FcRSoQeAfzXGTAXmAl8XkWnAfUChMWYiUBj5+KRyOcMV+vGLi2oa/Qmq0MM/nmZdXKSUskC3gW6MqTDGrI+8fwwoAUYCNwBLIk9bAtzYV4PsjNsZHr7v+EBv8DE4zkVF0PbUIm27KKVOfT3qoYvIWGAWsBrINcZUQDj0gWGdfM0iESkSkaLq6ur4Rnscd0uF3toSMcYktIcOegydUsoOMQe6iAwAXgK+aYypi/XrjDGLjTEFxpiCnJyc3oyxU9EKve1N0UZfEF8glJAeukePoVNKWSSmQBcRN+Ewf8oY83Lk4UoRyYt8Pg+o6pshds7VEuitFXp0UVFCK3RdXKSUskAss1wE+BNQYox5oM2nlgELI+8vBJYmfnhdc0fmmbc9KLq20Q/Ev0oU2lbo2nJRSp36Yjk9+QLgs8DHIrIx8ti/A/cDz4vIncA+4Ja+GWLnooc/H42EOLRunTs4Q2+KKqVOL90GujHm/4DOllwuSOxweiZ/cBoAB2qbWh6Lbp07OEHz0AGaNNCVUhaweqXoiEHhQC+raQ30RG2dC23moWugK6UsYHWge9xOcrNS2X+kseWx2kZfeGMuTwJaLjptUSllEasDHSB/cHr7Cr3Rx6D0FBxxbswFbQNdK3Sl1KkvCQI9jf01rRV6TYM/IatEAdI00JVSFkmKQK846m3Zz+VIgy8h/XNoM21R93JRSlnA+kAfNTidYMhwsM4LhI+fS8QMF9Bpi0opu1gf6PmD0wHYfyTcR09koDscQopTD7lQStkhCQI9OnWxEWNMuIeeoJYLQKrboRW6UsoK1gf6iEFpiITnojf4gviCIbITsEo0yuN20qx7uSilLGB9oKe4HAzP8rC/pjGhq0Sj9FxRpZQtrA90CLddymqaqGlM3CrRKI9LzxVVStkhKQJ91OB0DtQ0tSz7H5TQCl0DXSllh6QI9PBc9CaqjjUDCa7QteWilLJEkgR6OiEDxeXhg5QSsRd6lMft1AMulFJWSI5Azw5PXdxUVovTIWR6YtnmPTapLqdW6EopKyRFoI+KLC7aUl7HoDR3QjbmivK4Hbp9rlLKCkkR6MMHenAINAdCCV1UBHpTVCllj6QIdLfTQd7AcNslkf1ziNwUTcLNub701yIWryzt72EopRIoKQIdWrcASMRZom0l4zz04vI6lhdXsmJ7dX8PRSmVQN0Guoj8WUSqRGRzm8eyRWS5iOyIvB3ct8PsXnSTrkSuEoXWlosxJqGv259eWl8GtG5oppRKDrFU6I8DVx/32H1AoTFmIlAY+bhfjcqOVuiJb7mEDPiDyRHo/mCIpRsPAFBe20QwlBzXpZSKIdCNMSuBI8c9fAOwJPL+EuDGBI+rx6IVeuJ76JE90ZNkLvrK7dUcqvcxf8owAm32kVdK2a+3PfRcY0wFQOTtsM6eKCKLRKRIRIqqq/uuZ9vaQ09soKcm2TF0L60vY0hGCnfMHQ1AWZsDtpVSduvzm6LGmMXGmAJjTEFOTk6ffZ8Z+QO5fuYI5k0YktDX9bjCP6LmJFhcVNvo463iKq4/ewTjhg4AYH+N9tGVSha9XVJZKSJ5xpgKEckDqhI5qN5IT3Hx0G2zEv66niSq0F/dVIEvGOLm2fmMGORBBPZrha5U0uhthb4MWBh5fyGwNDHDOfW0Brr9FfpL68qYMjyTM0dkkepykpvpoUwrdKWSRizTFp8BPgAmi0iZiNwJ3A9cISI7gCsiHycljzv8I7L9puiB2iY27q/lxlkjEQlvjTAqO439NVqhK5Usum25GGNu6+RTCxI8llNStEJv8tkd6G9vDXfFLp+a2/JY/uB01uw+fgJT4uyoPIbDIUzIGdBn30Mp1SppVor2lZwBqQDsqq7v55HE5+2SSsYMSWdCTkbLY6Mi+8j7g4lvJzU0B7jt0dXc+9zGhL+2UqpjGujdGDs0g4nDBvD3zQf7eyi91uQLsqr0MPOnDGtpt0DrPvLltYnvoz/63i4O1TezubyOhuZAwl9fKXUiDfQYXHNWHmv2HKHqmJ2LcFaVHqI5EGL+lPbLBaL7yCf6xmjVMS+LV+4if3AawZBh/b6ahL6+UqpjGugxuPasPIyBN7ZU9vdQeqVwaxUZKU7mjMtu93h0H/lET118qHAHvkCIP9xxDg6BtX3Yp1dKtdJAj8Gk3AGMz8ng9U0V/T2UHjPG8HZJFRdNzCHV5Wz3ubyBHpwOSWiFXlpdzzNr9nP7eaOZPnIg00ZksWaPBrpSJ4MGegxEhGvPymP17sMcqg8fRB0Ihrj/71tZtfNQP4+ua8UVdRys8zJ/6om7M7icDvIGehI2dTEUMvz0tWI8LgffWDARgHPHZrNhXy2+JNxTXqlTjQZ6jK6ZnkfIwBtbwjdHf/q/JfxhRSk/WrbllN5a953IdMVLJ3e87UL+4LSYWi5ef5BlH5VTfay5w88HQ4Zvv/gR72yr5t4rJzM0MjtozthsmgMhPj5wtJdXkFiBYIhfvrGNMp1/r5KQBnqMpuZlMm5oBn//+CBPfriXx1ftYWpeFjuq6lmdwB5xIBjiql+v5K4la6lMwE6IhVurmJk/kGGZng4/P2pwekwtl5+9XsI3ntnA3J8V8sXH1/LapnJqGnwtY773+Y28vP4A37p8EndeOK7l6wrGhvv2RadI2+W9nYf43Ts7WbJqT38PJSb7jzRy7UPv8ccVpQT6YHqpSi4a6DESEa6ZPpxVpYf40bItzJ8yjBe/cj4D09w88eHehH2for01bKs8RuHWKq54YAUvry/r9b8AKuu8bNxfy/wpuZ0+J39wOlXHmrvcq2b1rsMs+WAvN80eyaKLx7Ol/Ch3P72BWT9ZzqW/eIebH1nF0o3lfPvKSdxz+cR2X5uTmcr4oRmsPUUC/dWPygEoLOn37Ydi8qs3t1FcUcfP/r6Vmx5ZRUlFXY9fo9nyVc4qdhroPfCJs8JtlzNyBvDgrWeTkerilnPyeWPzQaoStK/4m1sqSXE5ePXuC5mYm8m9z3/EnUuKejUT5Y8rduEQ4cZZIzp9TvRgkAOdzEVv8gX57kubGJWdxk9vnM53r57CqvsW8Nyiufzb1ZOZlJtJnTfA/7t2KnfPn9jha5w7Npu1e2oI9eAwDWMMOyqPJbQq9fqDvLmlkkyPi12HGig9xReLbSk/yt82lvPliyfw8O2zKa9t4rrf/h+LV5bG/Ev+h0s3c8H9b1Pb6Ovj0apTgQZ6D5w5IotHPjObJ+6cQ6YnfHbpZ+aOIRAyPLNmf9yvb4zhzeKDXHjGUKaPHMjzXz6f/3ftVD4oPcyVv17JH1eUxryqs+qYl6dW7+XGs0cyZkhGp88blX3i1MW2pxj96s1t7DncyP/cPIP0lPBOEU6HcN74IXzt0jNY/LkC3vn2pdx10fhOv8e547I52uRnR1X3AXqkwcdj7+3i8gdWcMWvV/LA8u3dfk2s3t1WTX1zgO9/YioAhSWn9jTUn/9jGwPT3Hz10glcOyOP5d+6hCum5fLfr2/lm89t7HYH0KdX7+OvH+zlUL2PJxP4r0h16urt9rmnJRHhmrPy2j02bmgGF00cytNr9vK1yybgdvb+d+TWg8coq2ni7svOAMLBeddF47nmrDx+tHQLP/v7Vt4qqeSJO89r2WOmM4+u3IU/GOLu+Wd0+bzowSD7a5poaA7wzec28lZJJcMyUxk5KI0N+2u5Y+5o5k0Y2uvrmhPpo6/Zc4TJwzNP+LwxhtW7j/D06n38Y/NBfMEQs0cPYs7YbP78/m4WzhtLblbH9wB64rVN5QzJSOFT5+Tz+Ko9vFVSxaKLJ7QbR9uVtCeDMYYPdh3m4Xd2sr2ynn+ZfwafOW8Mq3cdZsX2ar7/iakMTAsXD4MzUvj9Z2bz+3dL+eWb2yitrufz88bh9Qdp8gUZMySdy6YMw+10sG7vEX60bDMXT8rBGMPjq/Zy10Xju/3/RtlNAz0BPjt3DIueWEdhSSVXT8/r/guAj/bXsmTVHr591WRGDAqH6ptbKhGBBVPb97xHDkrjsYUFvLy+jH994SPufX4jv7ttNg5Hx+FzqL6ZJz7cyw1nj2Tc0M6rc4DcTA9up7Bhbw3Prd1HScUxPjt3DE2+IGU1TVwwYSj3XTM1pmvqzKjsNHKzUlm7+wifnTum5XFjDK9uquDBt7ZTWt1ApsfF7eeN5rY5o5k8PJN9hxtZ8MC7/OatHfzsprPiGkOjL0BhSRU3nzMSl9PB5VNzeWRFKbWNPgalp+ALhLj90Q8ZPSSdX90yM6HBvmJ7Nbuq68nN8kR+MRnKa72U1zbxxpaDrN9XS05mKmOy0/nh0i28tK6M5kCIEQM9fPb8Me1eS0T4+mVnMGV4Jt98diPffuGjdp8flpnKLQX5PF9UxohBafz21llsKT/K7Y+t5pUNB7htzugejf1okx8RyIr8izQZ9bcbP08AAA78SURBVMcv8r6igZ4AC6bmMnJQGr96czvnTxjaUlF1JBQyLH5vF798YxuBkKG6vpm/fnEOIsKbxQc5Z/RgcjJTO/zam2bnc7jex3+9XsLPs7dx3zVTOnzeoyt34Qt0X50DOBzCyEFpvLzhAOkpTh77XAGXTen0RMFeEREumDCUVzYeoLbJz23njmJoZir//XoJG/bVMjUvi198agafnDGCtJTWCnL0kHRunzOaJ1fv466LxnW4a+O2g8eob/ZzzpjsEz7X1lslVTT5g1w3I3w/YcHUYfzunZ28u62aG2eN5Pfv7qRobw1Fe2u4eGION84amZBrf2VDGd967qNOPz8qO42f3HAmtxSMItXlYNlH5fzktWIO1fv4xadmdFpRL5iay/vfm09tgx9PioNUl5O1u4/w1Oq9/P7dUtLcTp688zwGprs5f8IQpo/M4tH3dvHpglHtCoEjDT7W7D7Mhv21jB+awSWThjF8oIf9Rxp59L1dPLd2Pwa46szh/HNBPiMHpfHutmre2VZFIGj4n5tnMHpIekJ+ViebMYaH39nJw++UMml4JpdMyuGyyTnMGj243fMCwRD//srHnDs2m1sKRvXTaGOjgZ4ATofw80/NYOGf1/DVJ9fx+BfmkBI5um7tniO8VVyJ0yGkuBys2X2EVaWHuWb6cKaPHMgv3tjGC0VlzDtjCFvK6/j3T3Qc0lF3XTSOPYcb+MOKUkZnp3P7ee0rrso6L3/9YC/XzRwR87a100cOpMEX5C+fP5fpIwf27ofQjR9eN4387HReKNrPV59aD4RnwPz8UzO4eXY+zk7+tXH3/Im8sK6MB97czsOfmd3uc8+s2cePlm4Bgde/cSFnDGtt52w+cJQ/rtzFldNyufLMXF77qJzcrFTOjbR/ZuYPYuiAVJaXVDIlL5Pfvb2T62aOoLy2iR8u3cz5E4bE3eZ5Z1sV33lhE+ePH8Jvbj2bw/W+8FRUgRED08gb5Dmh8r3h7JFcOmkYq3cfbrfVcUeyPO52X3/5tFwun5ZLWU0j/qBp+deZiLDo4gl845kNFG6t4tLJObyy4QCPv7+H4sisGYdA9NbJhJwM9hxuxCFw06x8PG4Hf9tY3jJDKPqcQ/U+/un377P4cwWcMyYcgkeb/Gwqq2XehKGd/jc92YIhw2/e2s7G/bV85ZIJzJswhGDI8MNlW3h69T4unpTDMa+f3729g4cKd/Cty9vP1nqocAfPF5Xxtw3lTB85kKl5Wf14NV2Tk7kopqCgwBQVFZ2073eyvbQu3BK5adZIfvDJadz/9608V7Qft1MwBgIhQ0aKkx98chqfPncUxsBtj35IcUUdt583mj+u2MU737602zZJIBjiziVFrNxRzY9vmN7Sxqg65uX2R1dzoKaJ175xYcyB3uQL4nBwwtYAfSEYMqzYXkVZTRM3zc5nQGr3NcUDy7fzUOEOfnnLzJY59T/7ewnPrt3PhWcMZUv5UUZlp/PSV+fhdjqoOubl+t++T+UxL8ZAdkYKx7x+Pjt3LD+8blrL6373xU28/nEFY4dmUF7bxPJ7L+Fok59rHlzJ3PFD+Mvnzz3hn+JVx7xU1TVz5oisLv+ZvmFfDbc/uppxQzN47stzW26i95dAMMQlv3iXVLcDfzDE/iNNTMvL4toZecwdn81ZIwdRWl3Pu9uqWVV6iInDMvnSxePIGxhuB3r9Qd4qqaS20c/FE3MYPSSd0up6vvj4WiqOerlnwUS2lB/lrZIqfIEQny4Yxf03n9VtK+NwfTMPFe5g3b4a/AGDPxgixeVgUm4mk4dnMmv0IM4fP6Td6xw86uU3b21nQKqLaSOymDI8i7QUJ15/kOZAiNHZ6WRHDos/5vVzz7MbeXtrFVkeF3XeAHPGZpOW4mTF9mq+dukEvnPVZESE2kYfP361mJc3HODXn57JP83K54PSw9z+2Id8Ynoeq3cfYeiAFJbefcFJ+bvSloisM8YUdPs8DfTEeqhwBw8s306a24kvGOKuC8dxz+UTSU9xEQoZDLSrXPYcauDqB1fi9YeYOGwAy++9JKbv0+QLcvfT6yncWsU9CybymfNGc9ujH1Jx1MtfPn8u541P7GHZ/emY189Vv15J+dH2U0O/ftkE7r1iMm9sOcjXnlrPvVdM4iuXTOD2Rz9kc/lRXvzKPA43+Hh2zT4+2HWYZ740t1119eaWgyx6Yh0AD98+m2tnhO9//OX93fznq8Usung8k3Mzcbsc4cAvrmT9vhqMgWtn5PFfN05nUHo4OHyBEO+XHmL1riOs3XOETWW15A1M48Wvnt/poq6TbcmqPfxo2RZm5A/kngUTT9hOuTeONPj48hNFrN1Tw5CMFK6bOQJjDEs+2MuXLxnP99rcfzl41EuTP8jANDdpbidPrd7Lg4U7aPQFmTdhCBkpLtwuB/VeP9sr61um0l4/cwQ//afpZHncbCqr5a4lRRxt8gPQ3MGWEg6Bc8YMZv6UXF5eX8auQw38x3XTuKVgFM+t3c/D7+ykur6ZH19/Jp89f2y7r/UFQnzuz6tZv7eWh26bxY+WbSYj1cWrd1/I6t2H+eLjRSdcV1Rto4/S6gaaI79YAiFDlsfF4IwUBqW7yU5PwdXLSRMa6P3EGMNPXiuhuOIoP/jkNM4c0X0L40//t5ufvFbM1y+bwHeu6rrl0lYgGOJ7L3/MC+vKyPSEf2H85QtzTthVMRnUNwfYXnmM/UcaKatpYkb+QC6a2LqdwT3PbuB/N1VwyaQcCrdW8dvbZnHdzM7n30P4Rumc/yrkwjOG8sgds1vCLRQyfOHxtazYXt3u+dNHZnHltOGEjOF3b+9kyIAUvnPVFDYfOMqyj8o50uDD7RTOGjmQc8dms3De2JYb3qcCYww7q+o5Y9iAhN4E9AVC7KyqZ2LuANxOB8YYfrB0M09+uI/vXDWZ/MFpPLtmPx/sOnzC1148KYcfXDuVibknzn6q8/p5/P09PFi4g7yBHu6YO4bfvLWdIRmp/Pnz5zIhJ4PdhxrYVnmMQNCQ6nLgcjr4+MBR3iqupLiijkHpbn5/+2zmndE6S8vrD1JV19xp7/9oo5+bHnmf0uoGUpwOXvn6vJa/x997eRPPrt3Pf1x3JikuB7WNfvYcamDdvhp2djMt908LC06Y8BArDXSLBEOGp1fv5Zqz8lr2QImVMYZfvLGN54v284c7zmlZan+6qW30ceWvV1J1rJmvXjqB714d2y/G/UcayclMPeHmYyhkOFjnJRA0+IIhMj2udj31zQeO8q3nNrKjqp4Up4MrpuVy0+yRzJswtN2N3dNVKGS457mNLX33Udlp/PM5oxg5OI26Jj913gAz8gdyyaScbn+5rNtbwz3PbqCsponZowex+HMFMf09qTjaRHqKq8tJCp3Zd7iRu/66li9eMI5b28wMamgOcO1D77HncOu6jUHpbs4ZPZjZYwYzLS+L9BQnqW4nThGONvmpafRR2+hjwdTcXv+CPymBLiJXAw8CTuAxY0yXh0VroPedZJp61Vsb9tWwYns1/zJ/4km5Ief1B1m9+wgz8we2tF5UK18gxOOrdjMtbyDzJgzpdJptLOq8fgpLKrlmel6/z6Vv9AXYe7iRQeluBqWl4HE7+vzvXp8Huog4ge3AFUAZsBa4zRhT3NnXaKArpVTPxRro8Sz9nwPsNMbsMsb4gGeBG+J4PaWUUnGIJ9BHAm03MCmLPNaOiCwSkSIRKaqurj7+00oppRIknkDvqGl0Qv/GGLPYGFNgjCnIyen4kAWllFLxiyfQy4C262DzgfJOnquUUqqPxRPoa4GJIjJORFKAW4FliRmWUkqpnur1Xi7GmICI3A28QXja4p+NMVsSNjKllFI9EtfmXMaY14HXEzQWpZRScdATi5RSKkmc1KX/IlIN9PYsrKHAoQQOxyZ67aef0/W6Qa+9o2sfY4zpdprgSQ30eIhIUSwrpZKRXvvpd+2n63WDXns8164tF6WUShIa6EoplSRsCvTF/T2AfqTXfvo5Xa8b9Np7zZoeulJKqa7ZVKErpZTqgga6UkolCSsCXUSuFpFtIrJTRO7r7/H0FREZJSLviEiJiGwRkXsij2eLyHIR2RF5O7i/x9pXRMQpIhtE5LXIx6fFtYvIIBF5UUS2Rv77n386XLuIfCvy//pmEXlGRDzJfN0i8mcRqRKRzW0e6/R6ReR7kdzbJiJXdff6p3ygR05Gehi4BpgG3CYi0/p3VH0mAPyrMWYqMBf4euRa7wMKjTETgcLIx8nqHqCkzceny7U/CPzDGDMFmEn4Z5DU1y4iI4FvAAXGmOmE94S6leS+7seBq497rMPrjfzdvxU4M/I1v4/kYadO+UDnNDoZyRhTYYxZH3n/GOG/1CMJX++SyNOWADf2zwj7lojkA9cCj7V5OOmvXUSygIuBPwEYY3zGmFpOg2snvJ9Umoi4gHTCW3An7XUbY1YCR457uLPrvQF41hjTbIzZDewknIedsiHQYzoZKdmIyFhgFrAayDXGVEA49IFh/TeyPvUb4N+AUJvHTodrHw9UA3+JtJseE5EMkvzajTEHgF8C+4AK4Kgx5k2S/Lo70Nn19jj7bAj0mE5GSiYiMgB4CfimMaauv8dzMojIJ4EqY8y6/h5LP3ABs4FHjDGzgAaSq83QoUiv+AZgHDACyBCRO/p3VKeUHmefDYF+Wp2MJCJuwmH+lDHm5cjDlSKSF/l8HlDVX+PrQxcA14vIHsJttfki8iSnx7WXAWXGmNWRj18kHPDJfu2XA7uNMdXGGD/wMjCP5L/u43V2vT3OPhsC/bQ5GUlEhHAftcQY80CbTy0DFkbeXwgsPdlj62vGmO8ZY/KNMWMJ/zd+2xhzB6fHtR8E9ovI5MhDC4Bikv/a9wFzRSQ98v/+AsL3jZL9uo/X2fUuA24VkVQRGQdMBNZ0+UrGmFP+D/AJYDtQCny/v8fTh9d5IeF/Um0CNkb+fAIYQvju947I2+z+Hmsf/xwuBV6LvH9aXDtwNlAU+W//N2Dw6XDtwH8CW4HNwBNAajJfN/AM4fsFfsIV+J1dXS/w/UjubQOu6e71dem/UkolCRtaLkoppWKgga6UUklCA10ppZKEBrpSSiUJDXSllEoSGuhKKZUkNNCVUipJ/H+Ui71dTz6RsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "momentum = LinearReg(gd_type = 'momentum', eta = 3.5e-3)\n",
    "momentum.fit(X_train, y_train)\n",
    "plt.plot(momentum.loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бонус "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 7 (2 балла)**. Реализуйте линейную регрессию с функцией потерь MSE, обучаемую с помощью метода\n",
    "[Adam](https://arxiv.org/pdf/1412.6980.pdf) - добавьте при необходимости параметры в класс модели, повторите пункты 5 и 6 и сравните результаты. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 8 (2 балла)**. Реализуйте линейную регрессию с функцией потерь\n",
    "$$ L(\\hat{y}, y) = log(cosh(\\hat{y} - y)),$$\n",
    "\n",
    "обучаемую с помощью градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задание 9 (0.01 балла)**.  Вставьте картинку с вашим любимым мемом в этот Jupyter Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
